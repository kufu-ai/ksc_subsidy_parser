#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åˆ†é¡çµæœã‹ã‚‰å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®URLã‚’æŠ½å‡ºã™ã‚‹å°‚ç”¨ãƒ„ãƒ¼ãƒ«
"""

import pandas as pd
import json
import os
from pathlib import Path

def extract_individual_urls_from_classification(classification_file):
    """
    åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®URLã‚’æŠ½å‡º

    Args:
        classification_file (str): åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’åˆ¤å®šã—ã¦èª­ã¿è¾¼ã¿
        if classification_file.endswith('.json'):
            with open(classification_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
        elif classification_file.endswith('.csv'):
            df = pd.read_csv(classification_file)
            results = df.to_dict('records')
        else:
            raise ValueError("ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ï¼ˆ.json ã¾ãŸã¯ .csv ã®ã¿ï¼‰")

        # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        individual_pages = [r for r in results if r.get('page_type') == 'å€‹åˆ¥ãƒšãƒ¼ã‚¸']

        if not individual_pages:
            print("âŒ å€‹åˆ¥ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            return

        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        base_name = Path(classification_file).stem
        output_dir = Path(classification_file).parent

        individual_urls_file = output_dir / f"{base_name}_individual_urls.txt"
        individual_summary_file = output_dir / f"{base_name}_individual_summary.csv"
        individual_detailed_file = output_dir / f"{base_name}_individual_detailed.json"

        # 1. URLãƒªã‚¹ãƒˆã®ã¿ã‚’ä¿å­˜
        with open(individual_urls_file, 'w', encoding='utf-8') as f:
            for page in individual_pages:
                url = page.get('url', '')
                if url:
                    f.write(f"{url}\n")

        print(f"âœ… URLãƒªã‚¹ãƒˆä¿å­˜: {individual_urls_file} ({len(individual_pages)}ä»¶)")

        # 2. è©³ç´°æƒ…å ±ä»˜ãJSONä¿å­˜
        with open(individual_detailed_file, 'w', encoding='utf-8') as f:
            json.dump(individual_pages, f, ensure_ascii=False, indent=2)

        print(f"âœ… è©³ç´°æƒ…å ±ä¿å­˜: {individual_detailed_file}")

        # 3. è‡ªæ²»ä½“åˆ¥ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
        create_summary_by_prefecture(individual_pages, individual_summary_file)

        # 4. çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        display_statistics(individual_pages)

    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {str(e)}")

def create_summary_by_prefecture(individual_pages, summary_file):
    """
    éƒ½é“åºœçœŒãƒ»å¸‚åŒºç”ºæ‘åˆ¥ã®ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
    """
    try:
        # éƒ½é“åºœçœŒãƒ»å¸‚åŒºç”ºæ‘ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        grouped = {}
        for page in individual_pages:
            pref = page.get('prefecture', 'ä¸æ˜')
            city = page.get('city', 'ä¸æ˜')
            key = (pref, city)

            if key not in grouped:
                grouped[key] = {
                    'urls': [],
                    'titles': [],
                    'confidences': [],
                    'pages': []
                }

            grouped[key]['urls'].append(page.get('url', ''))
            grouped[key]['titles'].extend(page.get('found_subsidy_titles', []))
            grouped[key]['confidences'].append(page.get('confidence', 0.0))
            grouped[key]['pages'].append(page)

        # ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
        summary_data = []
        for (pref, city), data in grouped.items():
            unique_titles = list(set(data['titles']))  # é‡è¤‡é™¤å»
            avg_confidence = sum(data['confidences']) / len(data['confidences']) if data['confidences'] else 0.0

            summary_data.append({
                'éƒ½é“åºœçœŒå': pref,
                'å¸‚åŒºç”ºæ‘å': city,
                'å€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°': len(data['urls']),
                'å¹³å‡ç¢ºä¿¡åº¦': round(avg_confidence, 3),
                'è¦‹ã¤ã‹ã£ãŸè£œåŠ©é‡‘åˆ¶åº¦æ•°': len(unique_titles),
                'è£œåŠ©é‡‘åˆ¶åº¦ä¾‹': ', '.join(unique_titles[:5]),  # æœ€åˆã®5ã¤
                'URLä¾‹': data['urls'][0] if data['urls'] else '',
                'URLä¸€è¦§': '|'.join(data['urls'])  # ãƒ‘ã‚¤ãƒ—åŒºåˆ‡ã‚Šã§å…¨URL
            })

        # CSVã§ä¿å­˜
        df_summary = pd.DataFrame(summary_data)
        df_summary = df_summary.sort_values(['éƒ½é“åºœçœŒå', 'å¸‚åŒºç”ºæ‘å'])
        df_summary.to_csv(summary_file, index=False, encoding='utf-8')

        print(f"âœ… ã‚µãƒãƒªãƒ¼ä¿å­˜: {summary_file}")

    except Exception as e:
        print(f"âŒ ã‚µãƒãƒªãƒ¼ä½œæˆã‚¨ãƒ©ãƒ¼: {str(e)}")

def display_statistics(individual_pages):
    """
    çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    """
    print(f"\n{'='*50}")
    print(f"ğŸ“Š å€‹åˆ¥ãƒšãƒ¼ã‚¸æŠ½å‡ºçµæœçµ±è¨ˆ")
    print(f"{'='*50}")

    # åŸºæœ¬çµ±è¨ˆ
    print(f"ç·å€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°: {len(individual_pages)}")

    # éƒ½é“åºœçœŒåˆ¥çµ±è¨ˆ
    pref_counts = {}
    city_counts = {}

    for page in individual_pages:
        pref = page.get('prefecture', 'ä¸æ˜')
        city = page.get('city', 'ä¸æ˜')

        pref_counts[pref] = pref_counts.get(pref, 0) + 1
        city_key = f"{pref} {city}"
        city_counts[city_key] = city_counts.get(city_key, 0) + 1

    print(f"å¯¾è±¡éƒ½é“åºœçœŒæ•°: {len(pref_counts)}")
    print(f"å¯¾è±¡å¸‚åŒºç”ºæ‘æ•°: {len(city_counts)}")

    # ç¢ºä¿¡åº¦çµ±è¨ˆ
    confidences = [p.get('confidence', 0.0) for p in individual_pages]
    if confidences:
        avg_confidence = sum(confidences) / len(confidences)
        print(f"å¹³å‡ç¢ºä¿¡åº¦: {avg_confidence:.3f}")
        print(f"æœ€é«˜ç¢ºä¿¡åº¦: {max(confidences):.3f}")
        print(f"æœ€ä½ç¢ºä¿¡åº¦: {min(confidences):.3f}")

    # ä¸Šä½éƒ½é“åºœçœŒ
    print(f"\n--- å€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°ä¸Šä½5éƒ½é“åºœçœŒ ---")
    top_prefs = sorted(pref_counts.items(), key=lambda x: x[1], reverse=True)[:5]
    for pref, count in top_prefs:
        print(f"{pref}: {count}ä»¶")

    # ä¸Šä½å¸‚åŒºç”ºæ‘
    print(f"\n--- å€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°ä¸Šä½5å¸‚åŒºç”ºæ‘ ---")
    top_cities = sorted(city_counts.items(), key=lambda x: x[1], reverse=True)[:5]
    for city, count in top_cities:
        print(f"{city}: {count}ä»¶")

def find_classification_files():
    """
    åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    """
    classification_files = []

    # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    for pattern in ['*_page_classification.json', '*_page_classification.csv']:
        classification_files.extend(Path('.').glob(pattern))

    return sorted(classification_files)

def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    print("ğŸ” å€‹åˆ¥ãƒšãƒ¼ã‚¸URLæŠ½å‡ºãƒ„ãƒ¼ãƒ«")
    print("-" * 40)

    # åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    classification_files = find_classification_files()

    if not classification_files:
        print("âŒ åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("   (*_page_classification.json ã¾ãŸã¯ *_page_classification.csv)")
        return

    print("åˆ©ç”¨å¯èƒ½ãªåˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«:")
    for i, file in enumerate(classification_files, 1):
        print(f"{i}. {file.name}")

    try:
        choice = int(input("\næŠ½å‡ºã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ç•ªå·ã‚’é¸æŠã—ã¦ãã ã•ã„: ")) - 1
        if 0 <= choice < len(classification_files):
            selected_file = classification_files[choice]
            print(f"\nğŸ“‚ {selected_file.name} ã‹ã‚‰å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‚’æŠ½å‡ºä¸­...")
            extract_individual_urls_from_classification(str(selected_file))
        else:
            print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")

    except ValueError:
        print("âŒ ç„¡åŠ¹ãªå…¥åŠ›ã§ã™")
    except KeyboardInterrupt:
        print("\nâš ï¸  å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")

if __name__ == '__main__':
    main()
