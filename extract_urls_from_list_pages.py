#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰URLã‚’æŠ½å‡ºã—ã¦å†åˆ†é¡ã™ã‚‹ãƒ„ãƒ¼ãƒ«
"""

import pandas as pd
import json
import os
import time
import re
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from html_fetcher import fetch_html
from page_classifier import classify_page_type
from pathlib import Path

def extract_urls_from_html(html_content, base_url):
    """
    HTMLã‹ã‚‰URLã‚’æŠ½å‡ºã™ã‚‹

    Args:
        html_content (str): HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        base_url (str): ãƒ™ãƒ¼ã‚¹URL

    Returns:
        list: æŠ½å‡ºã•ã‚ŒãŸURLä¸€è¦§
    """
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        urls = set()

        # aã‚¿ã‚°ã‹ã‚‰hrefå±æ€§ã‚’æŠ½å‡º
        for link in soup.find_all('a', href=True):
            href = link.get('href')
            if href:
                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                absolute_url = urljoin(base_url, href)
                urls.add(absolute_url)

        return list(urls)

    except Exception as e:
        print(f"HTMLè§£æã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def filter_subsidy_related_urls(urls, keywords=None):
    """
    è£œåŠ©é‡‘é–¢é€£ã®URLã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

    Args:
        urls (list): URLä¸€è¦§
        keywords (list): é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰

    Returns:
        list: ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚ŒãŸURLä¸€è¦§
    """
    if keywords is None:
        # è£œåŠ©é‡‘é–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        include_keywords = [
            'è£œåŠ©', 'åŠ©æˆ', 'æ”¯æ´', 'äº¤ä»˜', 'çµ¦ä»˜', 'subsidy', 'grant', 'support',
            'åˆ¶åº¦', 'äº‹æ¥­', 'ç”³è«‹', 'å‹Ÿé›†'
        ]

        # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        exclude_keywords = [
            'javascript:', 'mailto:', 'tel:', '#',
            '.pdf', '.doc', '.xls', '.zip',
            'facebook', 'twitter', 'instagram', 'youtube',
            'login', 'admin', 'search', 'sitemap',
            'privacy', 'contact', 'about'
        ]
    else:
        include_keywords = keywords
        exclude_keywords = []

    filtered_urls = []

    for url in urls:
        url_lower = url.lower()

        # é™¤å¤–URLã‚’ã‚¹ã‚­ãƒƒãƒ—
        if any(keyword in url_lower for keyword in exclude_keywords):
            continue

        # è£œåŠ©é‡‘é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹URLã‚’å„ªå…ˆ
        if any(keyword in url_lower for keyword in include_keywords):
            filtered_urls.append(url)
        # åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³ã®HTMLãƒšãƒ¼ã‚¸ã‚‚å«ã‚ã‚‹ï¼ˆç›¸å¯¾URLãªã©ï¼‰
        elif url_lower.startswith('http') and not any(ext in url_lower for ext in ['.pdf', '.doc', '.xls', '.zip']):
            filtered_urls.append(url)

    # é‡è¤‡å‰Šé™¤
    return list(set(filtered_urls))

def extract_and_classify_from_list_pages(classification_results, max_urls_per_page=50, delay=2):
    """
    ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰URLã‚’æŠ½å‡ºã—ã¦åˆ†é¡ã™ã‚‹

    Args:
        classification_results (list): åˆ†é¡çµæœ
        max_urls_per_page (int): 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®æœ€å¤§URLæ•°
        delay (int): APIå‘¼ã³å‡ºã—é–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰

    Returns:
        dict: æŠ½å‡ºãƒ»åˆ†é¡çµæœ
    """
    # ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’æŠ½å‡º
    list_pages = [r for r in classification_results if r.get('page_type') == 'ä¸€è¦§ãƒšãƒ¼ã‚¸']

    if not list_pages:
        print("âŒ ä¸€è¦§ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return {'extracted_results': [], 'statistics': {}}

    print(f"ğŸ“‹ ä¸€è¦§ãƒšãƒ¼ã‚¸æ•°: {len(list_pages)}")

    all_extracted_results = []
    total_extracted_urls = 0

    for i, list_page in enumerate(list_pages, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“„ ä¸€è¦§ãƒšãƒ¼ã‚¸ {i}/{len(list_pages)}: {list_page.get('url', '')}")
        print(f"ğŸ›ï¸ {list_page.get('prefecture', '')} {list_page.get('city', '')}")
        print(f"{'='*60}")

        try:
            # HTMLã‚’å–å¾—
            filename = f"list_page_{int(time.time())}_{i}.html"
            html_path = fetch_html(list_page['url'], filename)

            # HTMLã‹ã‚‰URLã‚’æŠ½å‡º
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()

            extracted_urls = extract_urls_from_html(html_content, list_page['url'])
            print(f"ğŸ”— æŠ½å‡ºã•ã‚ŒãŸç·URLæ•°: {len(extracted_urls)}")

            # è£œåŠ©é‡‘é–¢é€£URLã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_urls = filter_subsidy_related_urls(extracted_urls)
            print(f"âœ… è£œåŠ©é‡‘é–¢é€£URLæ•°: {len(filtered_urls)}")

            # ä¸Šé™ã‚’é©ç”¨
            if len(filtered_urls) > max_urls_per_page:
                filtered_urls = filtered_urls[:max_urls_per_page]
                print(f"âš ï¸  ä¸Šé™é©ç”¨: {max_urls_per_page}ä»¶ã«åˆ¶é™")

            total_extracted_urls += len(filtered_urls)

            # å„URLã‚’åˆ†é¡
            for j, url in enumerate(filtered_urls, 1):
                print(f"  ğŸ” {j}/{len(filtered_urls)}: {url[:80]}...")

                try:
                    # URLåˆ†é¡
                    classification_result = classify_page_type(url)

                    # å…ƒã®ä¸€è¦§ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’è¿½åŠ 
                    classification_result.update({
                        'parent_list_page_url': list_page['url'],
                        'parent_prefecture': list_page.get('prefecture', ''),
                        'parent_city': list_page.get('city', ''),
                        'extraction_order': j,
                        'extracted_from_list': True
                    })

                    all_extracted_results.append(classification_result)

                    print(f"    ğŸ“ åˆ¤å®š: {classification_result.get('page_type', 'ä¸æ˜')} (ç¢ºä¿¡åº¦: {classification_result.get('confidence', 0.0):.2f})")

                    # APIè² è·è»½æ¸›ã®ãŸã‚å¾…æ©Ÿ
                    time.sleep(delay)

                except Exception as e:
                    print(f"    âŒ åˆ†é¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue

            # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            os.remove(html_path)

        except Exception as e:
            print(f"âŒ ä¸€è¦§ãƒšãƒ¼ã‚¸å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            continue

    # çµ±è¨ˆæƒ…å ±ã‚’ä½œæˆ
    statistics = create_extraction_statistics(all_extracted_results, list_pages)

    return {
        'extracted_results': all_extracted_results,
        'statistics': statistics,
        'total_list_pages': len(list_pages),
        'total_extracted_urls': total_extracted_urls
    }

def create_extraction_statistics(extracted_results, original_list_pages):
    """
    æŠ½å‡ºçµæœã®çµ±è¨ˆã‚’ä½œæˆ

    Args:
        extracted_results (list): æŠ½å‡ºãƒ»åˆ†é¡çµæœ
        original_list_pages (list): å…ƒã®ä¸€è¦§ãƒšãƒ¼ã‚¸

    Returns:
        dict: çµ±è¨ˆæƒ…å ±
    """
    stats = {
        'total_extracted': len(extracted_results),
        'by_page_type': {},
        'by_prefecture': {},
        'individual_pages_found': 0,
        'confidence_stats': {}
    }

    # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
    for result in extracted_results:
        page_type = result.get('page_type', 'ä¸æ˜')
        stats['by_page_type'][page_type] = stats['by_page_type'].get(page_type, 0) + 1

    # éƒ½é“åºœçœŒåˆ¥çµ±è¨ˆ
    for result in extracted_results:
        pref = result.get('parent_prefecture', 'ä¸æ˜')
        stats['by_prefecture'][pref] = stats['by_prefecture'].get(pref, 0) + 1

    # å€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°
    stats['individual_pages_found'] = stats['by_page_type'].get('å€‹åˆ¥ãƒšãƒ¼ã‚¸', 0)

    # ç¢ºä¿¡åº¦çµ±è¨ˆ
    confidences = [r.get('confidence', 0.0) for r in extracted_results if r.get('confidence') is not None]
    if confidences:
        stats['confidence_stats'] = {
            'average': sum(confidences) / len(confidences),
            'max': max(confidences),
            'min': min(confidences)
        }

    return stats

def save_extraction_results(results_data, base_filename):
    """
    æŠ½å‡ºçµæœã‚’ä¿å­˜

    Args:
        results_data (dict): æŠ½å‡ºçµæœãƒ‡ãƒ¼ã‚¿
        base_filename (str): ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
    """
    try:
        extracted_results = results_data['extracted_results']
        statistics = results_data['statistics']

        # ã™ã¹ã¦ã®çµæœã‚’ä¿å­˜
        all_results_file = f"{base_filename}_extracted_all.json"
        with open(all_results_file, 'w', encoding='utf-8') as f:
            json.dump(extracted_results, f, ensure_ascii=False, indent=2)
        print(f"âœ… å…¨æŠ½å‡ºçµæœä¿å­˜: {all_results_file}")

        # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®ã¿ã‚’æŠ½å‡º
        individual_pages = [r for r in extracted_results if r.get('page_type') == 'å€‹åˆ¥ãƒšãƒ¼ã‚¸']

        if individual_pages:
            # å€‹åˆ¥ãƒšãƒ¼ã‚¸URLãƒªã‚¹ãƒˆ
            individual_urls_file = f"{base_filename}_extracted_individual_urls.txt"
            with open(individual_urls_file, 'w', encoding='utf-8') as f:
                for page in individual_pages:
                    f.write(f"{page.get('url', '')}\n")
            print(f"âœ… æŠ½å‡ºå€‹åˆ¥ãƒšãƒ¼ã‚¸URLãƒªã‚¹ãƒˆ: {individual_urls_file} ({len(individual_pages)}ä»¶)")

            # å€‹åˆ¥ãƒšãƒ¼ã‚¸è©³ç´°
            individual_detailed_file = f"{base_filename}_extracted_individual_detailed.json"
            with open(individual_detailed_file, 'w', encoding='utf-8') as f:
                json.dump(individual_pages, f, ensure_ascii=False, indent=2)
            print(f"âœ… æŠ½å‡ºå€‹åˆ¥ãƒšãƒ¼ã‚¸è©³ç´°: {individual_detailed_file}")

            # CSVå½¢å¼ã§ã‚‚ä¿å­˜
            df_individual = pd.DataFrame(individual_pages)
            individual_csv_file = f"{base_filename}_extracted_individual.csv"
            df_individual.to_csv(individual_csv_file, index=False, encoding='utf-8')
            print(f"âœ… æŠ½å‡ºå€‹åˆ¥ãƒšãƒ¼ã‚¸CSV: {individual_csv_file}")

        # çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜
        stats_file = f"{base_filename}_extraction_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(statistics, f, ensure_ascii=False, indent=2)
        print(f"âœ… çµ±è¨ˆæƒ…å ±ä¿å­˜: {stats_file}")

        # çµ±è¨ˆè¡¨ç¤º
        print_extraction_statistics(statistics, results_data)

    except Exception as e:
        print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")

def print_extraction_statistics(statistics, results_data):
    """
    çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ä¸€è¦§ãƒšãƒ¼ã‚¸URLæŠ½å‡ºçµæœçµ±è¨ˆ")
    print(f"{'='*60}")

    print(f"å‡¦ç†ã—ãŸä¸€è¦§ãƒšãƒ¼ã‚¸æ•°: {results_data['total_list_pages']}")
    print(f"æŠ½å‡ºãƒ»åˆ†é¡ã—ãŸURLç·æ•°: {statistics['total_extracted']}")
    print(f"æ–°ãŸã«ç™ºè¦‹ã—ãŸå€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°: {statistics['individual_pages_found']}")

    print(f"\n--- ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ¥ ---")
    for page_type, count in statistics['by_page_type'].items():
        print(f"{page_type}: {count}ä»¶")

    print(f"\n--- éƒ½é“åºœçœŒåˆ¥ ---")
    sorted_prefs = sorted(statistics['by_prefecture'].items(), key=lambda x: x[1], reverse=True)
    for pref, count in sorted_prefs[:10]:  # ä¸Šä½10éƒ½é“åºœçœŒ
        print(f"{pref}: {count}ä»¶")

    if statistics['confidence_stats']:
        conf_stats = statistics['confidence_stats']
        print(f"\n--- ç¢ºä¿¡åº¦çµ±è¨ˆ ---")
        print(f"å¹³å‡: {conf_stats['average']:.3f}")
        print(f"æœ€é«˜: {conf_stats['max']:.3f}")
        print(f"æœ€ä½: {conf_stats['min']:.3f}")

def find_classification_files():
    """
    åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    """
    classification_files = []

    # åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    for pattern in ['*_page_classification.json', '*_page_classification.csv']:
        classification_files.extend(Path('.').glob(pattern))

    return sorted(classification_files)

def load_classification_results(file_path):
    """
    åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    """
    try:
        if file_path.endswith('.json'):
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        elif file_path.endswith('.csv'):
            df = pd.read_csv(file_path)
            return df.to_dict('records')
        else:
            raise ValueError("ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™")
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return []

def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    print("ğŸ”— ä¸€è¦§ãƒšãƒ¼ã‚¸URLæŠ½å‡ºãƒ»åˆ†é¡ãƒ„ãƒ¼ãƒ«")
    print("-" * 50)

    # åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    classification_files = find_classification_files()

    if not classification_files:
        print("âŒ åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("   (*_page_classification.json ã¾ãŸã¯ *_page_classification.csv)")
        return

    print("åˆ©ç”¨å¯èƒ½ãªåˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«:")
    for i, file in enumerate(classification_files, 1):
        print(f"{i}. {file.name}")

    try:
        choice = int(input("\nå‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ç•ªå·ã‚’é¸æŠã—ã¦ãã ã•ã„: ")) - 1
        if not (0 <= choice < len(classification_files)):
            print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
            return

        selected_file = classification_files[choice]
        print(f"\nğŸ“‚ {selected_file.name} ã‚’å‡¦ç†ä¸­...")

        # åˆ†é¡çµæœã‚’èª­ã¿è¾¼ã¿
        classification_results = load_classification_results(str(selected_file))

        if not classification_results:
            print("âŒ åˆ†é¡çµæœãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
            return

        # è¨­å®š
        max_urls = int(input("1ã¤ã®ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡ºã™ã‚‹æœ€å¤§URLæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 50): ") or "50")
        delay = int(input("APIå‘¼ã³å‡ºã—é–“éš”ï¼ˆç§’ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2): ") or "2")

        print(f"\nğŸš€ å‡¦ç†é–‹å§‹...")
        print(f"   - æœ€å¤§URLæ•°/ãƒšãƒ¼ã‚¸: {max_urls}")
        print(f"   - APIå‘¼ã³å‡ºã—é–“éš”: {delay}ç§’")

        # URLæŠ½å‡ºãƒ»åˆ†é¡å®Ÿè¡Œ
        results_data = extract_and_classify_from_list_pages(
            classification_results,
            max_urls_per_page=max_urls,
            delay=delay
        )

        # çµæœã‚’ä¿å­˜
        base_filename = selected_file.stem
        save_extraction_results(results_data, base_filename)

        print(f"\nğŸ‰ å‡¦ç†å®Œäº†ï¼")

    except ValueError:
        print("âŒ ç„¡åŠ¹ãªå…¥åŠ›ã§ã™")
    except KeyboardInterrupt:
        print("\nâš ï¸  å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")

if __name__ == '__main__':
    main()
