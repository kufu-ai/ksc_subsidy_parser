import json
import glob
import os
from html_fetcher import fetch_html
from openai_handler import process_html_file_with_openai
from csv_handler import save_to_csv
from utils import load_urls, get_output_path


def process_existing_urls():
    """æ—¢å­˜ã®URLãƒªã‚¹ãƒˆã‚’å‡¦ç†"""
    urls = load_urls()
    print(f"ğŸ“‹ æ—¢å­˜ã®URLãƒªã‚¹ãƒˆï¼ˆ{len(urls)}ä»¶ï¼‰ã®å‡¦ç†ã‚’é–‹å§‹...")

    for idx, url in enumerate(urls):
        print(f"\nğŸš€ {idx+1}/{len(urls)}: {url} ã®è§£æã‚’é–‹å§‹")

        html_path = fetch_html(url, f"page_{idx+1}.html")
        json_path = process_html_file_with_openai(html_path, url)

        # **â— JSONãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—**
        if json_path is None:
            print(f"âš ï¸ {url} ã®ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã‹ã£ãŸãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        save_to_csv(json_path)
        print(f"âœ… {url} ã®è§£æãŒå®Œäº†ã—ã¾ã—ãŸï¼\n")


def get_available_prefectures():
    """åˆ©ç”¨å¯èƒ½ãªéƒ½é“åºœçœŒãƒªã‚¹ãƒˆã‚’å–å¾—"""
    output_dir = get_output_path("")
    classification_files = glob.glob(
        os.path.join(output_dir, "*_all_classification.json")
    )

    prefectures = []
    for file_path in classification_files:
        filename = os.path.basename(file_path)
        # ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰éƒ½é“åºœçœŒåã‚’æŠ½å‡ºï¼ˆä¾‹ï¼šé•·é‡çœŒ_all_classification.json â†’ é•·é‡çœŒï¼‰
        prefecture = filename.replace("_all_classification.json", "")
        prefectures.append(prefecture)

    return sorted(prefectures)


def select_prefecture():
    """éƒ½é“åºœçœŒã‚’é¸æŠ"""
    prefectures = get_available_prefectures()

    if not prefectures:
        print("âš ï¸ *_all_classification.jsonãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return None

    print(f"\nğŸ“ {len(prefectures)}å€‹ã®éƒ½é“åºœçœŒã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
    print("=" * 50)

    for i, prefecture in enumerate(prefectures, 1):
        print(f"{i}. {prefecture}")

    print("=" * 50)

    while True:
        try:
            choice = input(
                f"å‡¦ç†ã—ãŸã„éƒ½é“åºœçœŒã®ç•ªå·ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ (1-{len(prefectures)}): "
            ).strip()
            choice_num = int(choice)

            if 1 <= choice_num <= len(prefectures):
                selected_prefecture = prefectures[choice_num - 1]
                print(f"âœ… {selected_prefecture} ã‚’é¸æŠã—ã¾ã—ãŸ")
                return selected_prefecture
            else:
                print(f"âš ï¸ 1ã‹ã‚‰{len(prefectures)}ã®ç¯„å›²ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        except ValueError:
            print("âš ï¸ æ•°å­—ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")


def process_classification_pages():
    """*_all_classification.jsonãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ä½å®…é–¢é€£å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®URLã‚’æŠ½å‡ºãƒ»å‡¦ç†"""
    # éƒ½é“åºœçœŒã‚’é¸æŠ
    selected_prefecture = select_prefecture()
    if not selected_prefecture:
        return

    print(f"\nğŸ“‹ {selected_prefecture}ã®ä½å®…é–¢é€£å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®URLã‚’æŠ½å‡ºãƒ»å‡¦ç†ã‚’é–‹å§‹...")

    # é¸æŠã•ã‚ŒãŸéƒ½é“åºœçœŒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    output_dir = get_output_path("")
    file_path = os.path.join(
        output_dir, f"{selected_prefecture}_all_classification.json"
    )

    if not os.path.exists(file_path):
        print(
            f"âš ï¸ {selected_prefecture}_all_classification.jsonãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        )
        return

    try:
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
    except Exception as e:
        print(f"âš ï¸ {file_path} ã®èª­ã¿è¾¼ã¿ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return

    # ä½å®…é–¢é€£å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®URLã‚’æŠ½å‡º
    target_urls = []
    for item in data:
        if (
            item.get("page_type") == "ä½å®…é–¢é€£å€‹åˆ¥ãƒšãƒ¼ã‚¸"
            and item.get("is_target_page") == "å¯¾è±¡"
        ):
            target_urls.append(item.get("url"))

    if not target_urls:
        print(
            f"âš ï¸ {selected_prefecture}ã§å¯¾è±¡ã¨ãªã‚‹ä½å®…é–¢é€£å€‹åˆ¥ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        )
        return

    print(
        f"ğŸ¯ {selected_prefecture}ã§{len(target_urls)}å€‹ã®ä½å®…é–¢é€£å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‚’ç™ºè¦‹ã—ã¾ã—ãŸ"
    )

    # å„URLã‚’å‡¦ç†
    for idx, url in enumerate(target_urls):
        print(f"\nğŸ  {idx+1}/{len(target_urls)}: {url} ã®è§£æã‚’é–‹å§‹")

        html_path = fetch_html(
            url, f"{selected_prefecture}_classification_page_{idx+1}.html"
        )
        json_path = process_html_file_with_openai(html_path, url)

        # **â— JSONãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—**
        if json_path is None:
            print(f"âš ï¸ {url} ã®ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã‹ã£ãŸãŸã‚ã€ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            continue

        save_to_csv(json_path)
        print(f"âœ… {url} ã®è§£æãŒå®Œäº†ã—ã¾ã—ãŸï¼\n")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("ğŸš€ è£œåŠ©é‡‘æƒ…å ±è§£æã‚·ã‚¹ãƒ†ãƒ ")
    print("=" * 50)
    print("1. æ—¢å­˜URLãƒªã‚¹ãƒˆï¼ˆurls.txtï¼‰ã®å‡¦ç†")
    print("2. åˆ†é¡æ¸ˆã¿ä½å®…é–¢é€£å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®å‡¦ç†")
    print("=" * 50)

    while True:
        choice = input("å‡¦ç†ã‚’é¸æŠã—ã¦ãã ã•ã„ (1 ã¾ãŸã¯ 2): ").strip()

        if choice == "1":
            process_existing_urls()
            break
        elif choice == "2":
            process_classification_pages()
            break
        else:
            print("âš ï¸ 1 ã¾ãŸã¯ 2 ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    print("\nğŸ‰ å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")


if __name__ == "__main__":
    main()
