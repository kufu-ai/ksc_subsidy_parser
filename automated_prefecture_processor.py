#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
éƒ½é“åºœçœŒåã‚’å…¥åŠ›ã™ã‚‹ã ã‘ã§å…¨ã¦ã®å‡¦ç†ã‚’è‡ªå‹•å®Ÿè¡Œã™ã‚‹çµ±åˆãƒ„ãƒ¼ãƒ«
å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼š
1. è£œåŠ©é‡‘URLæ¤œç´¢ (search_subsidy.py)
2. ãƒšãƒ¼ã‚¸åˆ†é¡ (page_classifier.py)
3. ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰URLæŠ½å‡º (extract_urls_from_list_pages.py)
4. çµæœãƒãƒ¼ã‚¸ (merge_classification_results.py)
5. æœ€çµ‚URLãƒªã‚¹ãƒˆç”Ÿæˆ
"""

import os
import sys
import time
import json
import pandas as pd
from pathlib import Path
from datetime import datetime

# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from search_subsidy import get_cities_by_prefecture, search_subsidy_urls, get_flexible_city_name, get_official_domain
from page_classifier import classify_urls_from_file, save_classification_results, extract_individual_page_urls
from extract_urls_from_list_pages import extract_and_classify_from_list_pages, save_extraction_results, load_classification_results
from merge_classification_results import merge_classification_results, create_comprehensive_summary, save_merged_results

def process_prefecture(prefecture_name, settings=None):
    """
    éƒ½é“åºœçœŒå…¨ä½“ã®å‡¦ç†ã‚’è‡ªå‹•å®Ÿè¡Œ

    Args:
        prefecture_name (str): éƒ½é“åºœçœŒå
        settings (dict): å‡¦ç†è¨­å®š

    Returns:
        dict: å‡¦ç†çµæœ
    """
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    default_settings = {
        'max_cities': None,  # None = å…¨å¸‚åŒºç”ºæ‘
        'max_urls_per_city': 20,
        'max_urls_per_list_page': 50,
        'classification_delay': 5,
        'extraction_delay': 5,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’5ç§’ã«å¤‰æ›´
        'use_openai_for_extraction': True,
        'save_intermediate_files': True
    }

    if settings:
        default_settings.update(settings)

    settings = default_settings

    print(f"ğŸš€ {prefecture_name} ã®è‡ªå‹•å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    print("=" * 60)

    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã®ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_filename = f"{prefecture_name}_{timestamp}"

    results = {
        'prefecture': prefecture_name,
        'timestamp': timestamp,
        'settings': settings,
        'step_results': {}
    }

    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: è£œåŠ©é‡‘URLæ¤œç´¢
        print(f"\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—1: {prefecture_name} ã®å¸‚åŒºç”ºæ‘åˆ¥è£œåŠ©é‡‘URLæ¤œç´¢")
        print("-" * 50)

        search_results = step1_search_subsidy_urls(prefecture_name, settings)
        results['step_results']['search'] = search_results

        if not search_results['success']:
            print(f"âŒ URLæ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {search_results['error']}")
            return results

        search_file = search_results['output_file']
        print(f"âœ… URLæ¤œç´¢å®Œäº†: {search_file} ({search_results['total_urls']}ä»¶ã®URL)")

        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒšãƒ¼ã‚¸åˆ†é¡
        print(f"\nğŸ” ã‚¹ãƒ†ãƒƒãƒ—2: ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ†é¡")
        print("-" * 50)

        classification_results = step2_classify_pages(search_file, settings)
        results['step_results']['classification'] = classification_results

        if not classification_results['success']:
            print(f"âŒ ãƒšãƒ¼ã‚¸åˆ†é¡ã«å¤±æ•—ã—ã¾ã—ãŸ: {classification_results['error']}")
            return results

        classification_file = classification_results['output_file']
        print(f"âœ… ãƒšãƒ¼ã‚¸åˆ†é¡å®Œäº†: {classification_file}")

        # ã‚¹ãƒ†ãƒƒãƒ—3: ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰URLæŠ½å‡º
        print(f"\nğŸ”— ã‚¹ãƒ†ãƒƒãƒ—3: ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰URLæŠ½å‡ºãƒ»åˆ†é¡")
        print("-" * 50)

        extraction_results = step3_extract_from_list_pages(classification_file, settings)
        results['step_results']['extraction'] = extraction_results

        if not extraction_results['success']:
            print(f"âŒ URLæŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: {extraction_results['error']}")
            return results

        print(f"âœ… URLæŠ½å‡ºå®Œäº†: {extraction_results['total_extracted']}ä»¶ã®æ–°è¦URL")

        # ã‚¹ãƒ†ãƒƒãƒ—4: çµæœãƒãƒ¼ã‚¸
        print(f"\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—4: çµæœãƒãƒ¼ã‚¸ãƒ»çµ±åˆ")
        print("-" * 50)

        merge_results = step4_merge_results(classification_file, extraction_results['data'], base_filename, settings)
        results['step_results']['merge'] = merge_results

        if not merge_results['success']:
            print(f"âŒ çµæœãƒãƒ¼ã‚¸ã«å¤±æ•—ã—ã¾ã—ãŸ: {merge_results['error']}")
            return results

        final_file = merge_results['final_url_file']
        print(f"âœ… çµæœãƒãƒ¼ã‚¸å®Œäº†: {final_file} ({merge_results['total_individual_pages']}ä»¶ã®å€‹åˆ¥ãƒšãƒ¼ã‚¸)")

        # æœ€çµ‚ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print_final_summary(results)

        results['success'] = True
        results['final_url_file'] = final_file

        return results

    except Exception as e:
        error_msg = f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        print(f"âŒ {error_msg}")
        results['success'] = False
        results['error'] = error_msg
        return results

def step1_search_subsidy_urls(prefecture_name, settings):
    """
    ã‚¹ãƒ†ãƒƒãƒ—1: è£œåŠ©é‡‘URLæ¤œç´¢
    """
    try:
        from search_subsidy import get_cities_by_prefecture, search_subsidy_urls

        # å¸‚åŒºç”ºæ‘ãƒªã‚¹ãƒˆã‚’å–å¾—
        cities = get_cities_by_prefecture(prefecture_name)
        print(f"å¯¾è±¡å¸‚åŒºç”ºæ‘æ•°: {len(cities)}")

        # ä¸Šé™è¨­å®šã®é©ç”¨
        if settings['max_cities'] and len(cities) > settings['max_cities']:
            cities = cities[:settings['max_cities']]
            print(f"âš ï¸  å‡¦ç†ã‚’ {settings['max_cities']} å¸‚åŒºç”ºæ‘ã«åˆ¶é™")

        result_list = []
        total_urls = 0

        for i, city in enumerate(cities, 1):
            print(f"  {i}/{len(cities)}: {city} ã‚’æ¤œç´¢ä¸­...")

            # search_subsidy_urlså†…ã§æŸ”è»Ÿãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã•ã‚Œã‚‹
            urls = search_subsidy_urls(city, prefecture_name, max_results=settings['max_urls_per_city'])

            result_list.append({
                "éƒ½é“åºœçœŒå": prefecture_name,
                "å¸‚åŒºç”ºæ‘å": city,
                "è£œåŠ©é‡‘é–¢é€£URL": urls
            })
            total_urls += len(urls)
            print(f"    ğŸ“ {len(urls)}ä»¶ã®URLã‚’å–å¾—")

            # APIè² è·è»½æ¸›
            time.sleep(1)

            #TODO: kesu é–‹ç™ºä¸­ã¯2ä»¶ã§ã‚¹ã‚­ãƒƒãƒ—
            if i >= 2:
                print(f"    âš ï¸  é–‹ç™ºãƒ¢ãƒ¼ãƒ‰: {i}ä»¶ã§å‡¦ç†ã‚’åœæ­¢")
                break

        # çµæœã‚’ä¿å­˜
        output_json = f"{prefecture_name}_subsidy_urls.json"
        output_csv = f"{prefecture_name}_subsidy_urls.csv"

        df_result = pd.DataFrame(result_list)
        df_result.to_json(output_json, force_ascii=False, orient="records", indent=2)
        df_result.to_csv(output_csv, index=False, encoding='utf-8')

        return {
            'success': True,
            'output_file': output_json,
            'total_cities': len(cities),
            'total_urls': total_urls
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

def step2_classify_pages(search_file, settings):
    """
    ã‚¹ãƒ†ãƒƒãƒ—2: ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ†é¡
    """
    try:
        # åˆ†é¡å®Ÿè¡Œ
        classification_results = classify_urls_from_file(search_file)

        if not classification_results:
            return {
                'success': False,
                'error': 'ãƒšãƒ¼ã‚¸åˆ†é¡çµæœãŒç©ºã§ã™'
            }

        # çµæœã‚’ä¿å­˜
        base_filename = Path(search_file).stem
        output_csv = f"{base_filename}_page_classification.csv"
        save_classification_results(classification_results, output_csv)

        # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‚‚æŠ½å‡º
        extract_individual_page_urls(classification_results, output_csv)

        output_file = f"{base_filename}_page_classification.json"

        # çµ±è¨ˆ
        page_types = {}
        for result in classification_results:
            page_type = result.get('page_type', 'ä¸æ˜')
            page_types[page_type] = page_types.get(page_type, 0) + 1

        return {
            'success': True,
            'output_file': output_file,
            'total_classified': len(classification_results),
            'page_types': page_types
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

def step3_extract_from_list_pages(classification_file, settings):
    """
    ã‚¹ãƒ†ãƒƒãƒ—3: ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰URLæŠ½å‡º
    """
    try:
        # åˆ†é¡çµæœã‚’èª­ã¿è¾¼ã¿
        classification_results = load_classification_results(classification_file)

        if not classification_results:
            return {
                'success': False,
                'error': 'åˆ†é¡çµæœã®èª­ã¿è¾¼ã¿ã«å¤±æ•—'
            }

        # æŠ½å‡ºæ–¹æ³•ã‚’è¨­å®š
        extraction_method = "openai" if settings['use_openai_for_extraction'] else "improved"

        # URLæŠ½å‡ºãƒ»åˆ†é¡å®Ÿè¡Œ
        extraction_data = extract_and_classify_from_list_pages(
            classification_results,
            max_urls_per_page=settings['max_urls_per_list_page'],
            delay=settings['extraction_delay'],
            extraction_method=extraction_method
        )

        # çµæœã‚’ä¿å­˜
        base_filename = Path(classification_file).stem
        save_extraction_results(extraction_data, base_filename)

        return {
            'success': True,
            'data': extraction_data,
            'total_extracted': extraction_data['total_extracted_urls']
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

def step4_merge_results(classification_file, extraction_data, base_filename, settings):
    """
    ã‚¹ãƒ†ãƒƒãƒ—4: çµæœãƒãƒ¼ã‚¸
    """
    try:
        # å…ƒã®åˆ†é¡çµæœã‚’èª­ã¿è¾¼ã¿
        original_results = load_classification_results(classification_file)
        extracted_results = extraction_data['extracted_results']

        # ãƒãƒ¼ã‚¸å®Ÿè¡Œ
        merged_data = merge_classification_results(original_results, extracted_results)

        # åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼ä½œæˆ
        comprehensive_summary = create_comprehensive_summary(merged_data)

        # çµæœã‚’ä¿å­˜
        save_merged_results(merged_data, comprehensive_summary, base_filename)

        final_url_file = f"{base_filename}_merged_individual_urls.txt"

        return {
            'success': True,
            'final_url_file': final_url_file,
            'total_individual_pages': merged_data['statistics']['merged_count'],
            'statistics': merged_data['statistics']
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

def print_final_summary(results):
    """
    æœ€çµ‚ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    """
    print(f"\n{'='*60}")
    print(f"ğŸ‰ {results['prefecture']} ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"{'='*60}")

    # å„ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœ
    steps = ['search', 'classification', 'extraction', 'merge']
    step_names = ['URLæ¤œç´¢', 'ãƒšãƒ¼ã‚¸åˆ†é¡', 'URLæŠ½å‡º', 'çµæœãƒãƒ¼ã‚¸']

    for step, name in zip(steps, step_names):
        step_result = results['step_results'].get(step, {})
        if step_result.get('success'):
            print(f"âœ… {name}: æˆåŠŸ")
        else:
            print(f"âŒ {name}: å¤±æ•—")

    # æœ€çµ‚çµ±è¨ˆ
    if results.get('success'):
        merge_stats = results['step_results']['merge']['statistics']
        print(f"\nğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
        print(f"  - åˆå›æ¤œç´¢ç”±æ¥ã®å€‹åˆ¥ãƒšãƒ¼ã‚¸: {merge_stats['original_count']}ä»¶")
        print(f"  - ä¸€è¦§ãƒšãƒ¼ã‚¸æŠ½å‡ºç”±æ¥: {merge_stats['extracted_count']}ä»¶")
        print(f"  - é‡è¤‡é™¤å»å¾Œã®ç·å€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°: {merge_stats['merged_count']}ä»¶")
        print(f"  - æ–°è¦ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°: {merge_stats['new_from_extraction']}ä»¶")

        print(f"\nğŸ’¾ æœ€çµ‚URLãƒªã‚¹ãƒˆ: {results['final_url_file']}")

def interactive_prefecture_processor():
    """
    å¯¾è©±çš„ãªéƒ½é“åºœçœŒå‡¦ç†
    """
    print("ğŸ›ï¸ éƒ½é“åºœçœŒåˆ¥è£œåŠ©é‡‘URLçµ±åˆå‡¦ç†ãƒ„ãƒ¼ãƒ«")
    print("=" * 50)

    # éƒ½é“åºœçœŒåã‚’å…¥åŠ›
    prefecture_name = input("å‡¦ç†ã™ã‚‹éƒ½é“åºœçœŒåã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()

    if not prefecture_name:
        print("âŒ éƒ½é“åºœçœŒåãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    # è¨­å®šã‚’ç¢ºèª
    print(f"\nâš™ï¸  å‡¦ç†è¨­å®š:")
    print(f"1. URLæŠ½å‡ºã«OpenAI APIã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰")
    print(f"2. å‡¦ç†ã™ã‚‹å¸‚åŒºç”ºæ‘æ•°: å…¨ã¦")
    print(f"3. APIå‘¼ã³å‡ºã—é–“éš”: åˆ†é¡5ç§’ã€æŠ½å‡º5ç§’")

    use_custom = input("\nè¨­å®šã‚’å¤‰æ›´ã—ã¾ã™ã‹ï¼Ÿ (y/N): ").strip().lower()

    settings = {}
    if use_custom == 'y':
        try:
            max_cities = input("å‡¦ç†ã™ã‚‹æœ€å¤§å¸‚åŒºç”ºæ‘æ•° (ç©ºç™½=å…¨ã¦): ").strip()
            settings['max_cities'] = int(max_cities) if max_cities else None

            use_openai = input("URLæŠ½å‡ºã«OpenAI APIã‚’ä½¿ç”¨ã—ã¾ã™ã‹ï¼Ÿ (Y/n): ").strip().lower()
            settings['use_openai_for_extraction'] = use_openai != 'n'

            classification_delay = input("åˆ†é¡APIå‘¼ã³å‡ºã—é–“éš”ï¼ˆç§’ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5): ").strip()
            settings['classification_delay'] = int(classification_delay) if classification_delay else 5

            extraction_delay = input("æŠ½å‡ºAPIå‘¼ã³å‡ºã—é–“éš”ï¼ˆç§’ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5): ").strip()
            settings['extraction_delay'] = int(extraction_delay) if extraction_delay else 5

        except ValueError:
            print("âš ï¸  ç„¡åŠ¹ãªå…¥åŠ›ãŒã‚ã‚Šã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            settings = {}

    # ç¢ºèª
    print(f"\nğŸš€ {prefecture_name} ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
    confirm = input("ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿ (Y/n): ").strip().lower()

    if confirm == 'n':
        print("âŒ å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
        return

    # å‡¦ç†å®Ÿè¡Œ
    start_time = time.time()
    results = process_prefecture(prefecture_name, settings)
    end_time = time.time()

    # å‡¦ç†æ™‚é–“è¡¨ç¤º
    elapsed_time = end_time - start_time
    print(f"\nâ±ï¸  ç·å‡¦ç†æ™‚é–“: {elapsed_time/60:.1f}åˆ†")

    if results.get('success'):
        print(f"ğŸ‰ å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"ğŸ“ æœ€çµ‚çµæœ: {results['final_url_file']}")
    else:
        print(f"âŒ å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {results.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")

def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    try:
        if len(sys.argv) > 1:
            # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰éƒ½é“åºœçœŒåã‚’å–å¾—
            prefecture_name = sys.argv[1]
            results = process_prefecture(prefecture_name)
        else:
            # å¯¾è©±çš„ãƒ¢ãƒ¼ãƒ‰
            interactive_prefecture_processor()

    except KeyboardInterrupt:
        print("\nâš ï¸  å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")

if __name__ == '__main__':
    main()