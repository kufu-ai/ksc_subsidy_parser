#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
éƒ½é“åºœçœŒåã‚’å…¥åŠ›ã™ã‚‹ã ã‘ã§å…¨ã¦ã®å‡¦ç†ã‚’è‡ªå‹•å®Ÿè¡Œã™ã‚‹çµ±åˆãƒ„ãƒ¼ãƒ«
å‡¦ç†ãƒ•ãƒ­ãƒ¼ï¼š
1. è£œåŠ©é‡‘URLæ¤œç´¢ (search_subsidy.py)
2. ãƒšãƒ¼ã‚¸åˆ†é¡ (page_classifier.py)
3. ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰URLæŠ½å‡º (extract_urls_from_list_pages.py)
4. çµæœãƒãƒ¼ã‚¸ (merge_classification_results.py)
5. æœ€çµ‚URLãƒªã‚¹ãƒˆç”Ÿæˆ
"""

import os
import sys
import time
import json
import pandas as pd
from pathlib import Path
from datetime import datetime

# å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from search_subsidy import get_cities_by_prefecture, search_subsidy_urls, get_flexible_city_name, get_official_domain
from page_classifier import classify_urls_from_file, save_classification_results
from extract_urls_from_list_pages import extract_and_classify_from_list_pages, save_extraction_results, load_classification_results
from merge_classification_results import merge_classification_results, create_comprehensive_summary, save_merged_results

def process_prefecture(prefecture_name, settings=None):
    """
    éƒ½é“åºœçœŒå…¨ä½“ã®å‡¦ç†ã‚’è‡ªå‹•å®Ÿè¡Œ

    Args:
        prefecture_name (str): éƒ½é“åºœçœŒå
        settings (dict): å‡¦ç†è¨­å®š

    Returns:
        dict: å‡¦ç†çµæœ
    """
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
    default_settings = {
        'max_cities': None,  # None = å…¨å¸‚åŒºç”ºæ‘
        'max_urls_per_city': 10,
        'max_urls_per_list_page': 50,
        'classification_delay': 5,
        'extraction_delay': 5,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’5ç§’ã«å¤‰æ›´
        'use_openai_for_extraction': True,
        'save_intermediate_files': True
    }

    if settings:
        default_settings.update(settings)

    settings = default_settings

    print(f"ğŸš€ {prefecture_name} ã®è‡ªå‹•å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
    print("=" * 60)

    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãã®ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_filename = f"{prefecture_name}_{timestamp}"

    results = {
        'prefecture': prefecture_name,
        'timestamp': timestamp,
        'settings': settings,
        'step_results': {}
    }

    try:
        # ã‚¹ãƒ†ãƒƒãƒ—1: è£œåŠ©é‡‘URLæ¤œç´¢
        print(f"\nğŸ“‹ ã‚¹ãƒ†ãƒƒãƒ—1: {prefecture_name} ã®å¸‚åŒºç”ºæ‘åˆ¥è£œåŠ©é‡‘URLæ¤œç´¢")
        print("-" * 50)

        search_results = step1_search_subsidy_urls(prefecture_name, settings)
        results['step_results']['search'] = search_results

        if not search_results['success']:
            print(f"âŒ URLæ¤œç´¢ã«å¤±æ•—ã—ã¾ã—ãŸ: {search_results['error']}")
            return results

        search_file = search_results['output_file']
        print(f"âœ… URLæ¤œç´¢å®Œäº†: {search_file} ({search_results['total_urls']}ä»¶ã®URL)")

        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒšãƒ¼ã‚¸åˆ†é¡
        print(f"\nğŸ” ã‚¹ãƒ†ãƒƒãƒ—2: ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ†é¡")
        print("-" * 50)

        classification_results = step2_classify_pages(search_file, settings)
        results['step_results']['classification'] = classification_results

        if not classification_results['success']:
            print(f"âŒ ãƒšãƒ¼ã‚¸åˆ†é¡ã«å¤±æ•—ã—ã¾ã—ãŸ: {classification_results['error']}")
            return results

        classification_file = classification_results['output_file']
        print(f"âœ… ãƒšãƒ¼ã‚¸åˆ†é¡å®Œäº†: {classification_file}")

        # ã‚¹ãƒ†ãƒƒãƒ—3: ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰URLæŠ½å‡º
        print(f"\nğŸ”— ã‚¹ãƒ†ãƒƒãƒ—3: ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰URLæŠ½å‡ºãƒ»åˆ†é¡")
        print("-" * 50)

        extraction_results = step3_extract_from_list_pages(classification_file, settings)
        results['step_results']['extraction'] = extraction_results

        if not extraction_results['success']:
            print(f"âŒ URLæŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: {extraction_results['error']}")
            return results

        print(f"âœ… URLæŠ½å‡ºå®Œäº†: {extraction_results['total_extracted']}ä»¶ã®æ–°è¦URL")

        # ã‚¹ãƒ†ãƒƒãƒ—4: çµæœãƒãƒ¼ã‚¸
        print(f"\nğŸ”„ ã‚¹ãƒ†ãƒƒãƒ—4: çµæœãƒãƒ¼ã‚¸ãƒ»çµ±åˆ")
        print("-" * 50)

        merge_results = step4_merge_results(classification_file, extraction_results['data'], base_filename, settings)
        results['step_results']['merge'] = merge_results

        if not merge_results['success']:
            print(f"âŒ çµæœãƒãƒ¼ã‚¸ã«å¤±æ•—ã—ã¾ã—ãŸ: {merge_results['error']}")
            return results

        final_file = merge_results['final_url_file']
        print(f"âœ… çµæœãƒãƒ¼ã‚¸å®Œäº†: {final_file} ({merge_results['total_individual_pages']}ä»¶ã®å€‹åˆ¥ãƒšãƒ¼ã‚¸)")

        # æœ€çµ‚ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        print_final_summary(results)

        results['success'] = True
        results['final_url_file'] = final_file

        return results

    except Exception as e:
        error_msg = f"å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"
        print(f"âŒ {error_msg}")
        results['success'] = False
        results['error'] = error_msg
        return results

def step1_search_subsidy_urls(prefecture_name, settings):
    """
    ã‚¹ãƒ†ãƒƒãƒ—1: è£œåŠ©é‡‘URLæ¤œç´¢
    """
    try:
        from search_subsidy import get_cities_by_prefecture, search_subsidy_urls

        # å¸‚åŒºç”ºæ‘ãƒªã‚¹ãƒˆã‚’å–å¾—
        cities = get_cities_by_prefecture(prefecture_name)
        print(f"å¯¾è±¡å¸‚åŒºç”ºæ‘æ•°: {len(cities)}")

        # ä¸Šé™è¨­å®šã®é©ç”¨
        if settings['max_cities'] and len(cities) > settings['max_cities']:
            cities = cities[:settings['max_cities']]
            print(f"âš ï¸  å‡¦ç†ã‚’ {settings['max_cities']} å¸‚åŒºç”ºæ‘ã«åˆ¶é™")

        result_list = []
        total_urls = 0

        for i, city in enumerate(cities, 1):
            if city == "åƒè‘‰å¸‚" or city == "éŠšå­å¸‚":
                print(f"  {i}/{len(cities)}: {city} ã‚’æ¤œç´¢ä¸­...")

                # search_subsidy_urlså†…ã§æŸ”è»Ÿãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã•ã‚Œã‚‹
                urls = search_subsidy_urls(city, prefecture_name, max_results=settings['max_urls_per_city'])

                result_list.append({
                    "éƒ½é“åºœçœŒå": prefecture_name,
                    "city_name": city,
                    "è£œåŠ©é‡‘é–¢é€£URL": urls
                })
                total_urls += len(urls)
                print(f"    ğŸ“ {len(urls)}ä»¶ã®URLã‚’å–å¾—")

                # APIè² è·è»½æ¸›
                time.sleep(1)

                #TODO: kesu é–‹ç™ºä¸­ã¯2ä»¶ã§ã‚¹ã‚­ãƒƒãƒ—
                # if i >= 2:
                #     print(f"    âš ï¸  é–‹ç™ºãƒ¢ãƒ¼ãƒ‰: {i}ä»¶ã§å‡¦ç†ã‚’åœæ­¢")
                #     break

        # çµæœã‚’ä¿å­˜
        output_json = f"{prefecture_name}_subsidy_urls.json"
        output_csv = f"{prefecture_name}_subsidy_urls.csv"

        df_result = pd.DataFrame(result_list)
        df_result.to_json(output_json, force_ascii=False, orient="records", indent=2)
        df_result.to_csv(output_csv, index=False, encoding='utf-8')

        return {
            'success': True,
            'output_file': output_json,
            'total_cities': len(cities),
            'total_urls': total_urls
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

def step2_classify_pages(search_file, settings):
    """
    ã‚¹ãƒ†ãƒƒãƒ—2: ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ†é¡
    """
    try:
        # åˆ†é¡å®Ÿè¡Œ
        classification_results = classify_urls_from_file(search_file)

        if not classification_results:
            return {
                'success': False,
                'error': 'ãƒšãƒ¼ã‚¸åˆ†é¡çµæœãŒç©ºã§ã™'
            }

        # çµæœã‚’ä¿å­˜
        base_filename = Path(search_file).stem
        output_file = f"{base_filename}_page_classification.json"
        save_classification_results(classification_results, output_file)

        # çµ±è¨ˆ
        page_types = {}
        for result in classification_results:
            page_type = result.get('page_type', 'ä¸æ˜')
            page_types[page_type] = page_types.get(page_type, 0) + 1

        return {
            'success': True,
            'output_file': output_file,
            'total_classified': len(classification_results),
            'page_types': page_types
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

def step3_extract_from_list_pages(classification_file, settings):
    """
    ã‚¹ãƒ†ãƒƒãƒ—3: ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰URLæŠ½å‡º
    found_new_housing_subsidiesã‚’ä½¿ç”¨
    ã•ã‚‰ã«ãƒªãƒ³ã‚¯å…ˆãŒä¸€è¦§ãƒšãƒ¼ã‚¸ã®å ´åˆã¯ç„¡è¦–ã™ã‚‹ã€‚
    """
    try:
        # åˆ†é¡çµæœã‚’èª­ã¿è¾¼ã¿
        classification_results = load_classification_results(classification_file)

        if not classification_results:
            return {
                'success': False,
                'error': 'åˆ†é¡çµæœã®èª­ã¿è¾¼ã¿ã«å¤±æ•—'
            }

        # ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã¤found_new_housing_subsidiesãŒå­˜åœ¨ã™ã‚‹ãƒšãƒ¼ã‚¸ã‚’æŠ½å‡º
        list_pages_with_subsidies = []
        for result in classification_results:
            if (result.get('page_type') == 'æ–°ç¯‰ä½å®…é–¢é€£ä¸€è¦§ãƒšãƒ¼ã‚¸' and
                result.get('found_new_housing_subsidies') and
                len(result.get('found_new_housing_subsidies', [])) > 0):
                list_pages_with_subsidies.append(result)

        if not list_pages_with_subsidies:
            print("âš ï¸ è£œåŠ©é‡‘ãŒè¦‹ã¤ã‹ã£ãŸä¸€è¦§ãƒšãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
            all_extracted_results = []
            statistics = create_extraction_statistics_from_found_subsidies(all_extracted_results, [])
            base_filename = Path(classification_file).stem
            save_extraction_results_from_found_subsidies(all_extracted_results, statistics, base_filename)
            return {
                'success': True,
                'data': {
                    'extracted_results': all_extracted_results,
                    'statistics': statistics,
                    'total_list_pages': 0,
                    'total_extracted_urls': 0
                },
                'total_extracted': 0
            }

        print(f"ğŸ“‹ è£œåŠ©é‡‘ç™ºè¦‹æ¸ˆã¿ä¸€è¦§ãƒšãƒ¼ã‚¸æ•°: {len(list_pages_with_subsidies)}")

        all_extracted_results = []
        total_extracted_urls = 0

        for i, list_page in enumerate(list_pages_with_subsidies, 1):
            print(f"\n{'='*60}")
            print(f"ğŸ“„ ä¸€è¦§ãƒšãƒ¼ã‚¸ {i}/{len(list_pages_with_subsidies)}: {list_page.get('url', '')}")
            print(f"ğŸ›ï¸ {list_page.get('prefecture', '')} {list_page.get('city', '')}")

            found_subsidies = list_page.get('found_new_housing_subsidies', [])
            print(f"ğŸ”— ç™ºè¦‹æ¸ˆã¿è£œåŠ©é‡‘æ•°: {len(found_subsidies)}")
            print(f"{'='*60}")

            # found_new_housing_subsidiesã‹ã‚‰å„URLã‚’åˆ†é¡
            for j, subsidy_info in enumerate(found_subsidies, 1):
                url = subsidy_info.get('url', '')
                title = subsidy_info.get('title', '')

                if not url:
                    print(f"  âš ï¸  {j}/{len(found_subsidies)}: URLãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ— - {title}")
                    continue

                print(f"  ğŸ” {j}/{len(found_subsidies)}: {title}")
                print(f"      URL: {url[:80]}...")

                try:
                    # URLã‚’åˆ†é¡ï¼ˆpage_classifier.pyã®classify_page_typeã‚’ä½¿ç”¨ï¼‰
                    from page_classifier import classify_page_type
                    classification_result = classify_page_type(url)

                    # å…ƒã®ä¸€è¦§ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’è¿½åŠ 
                    classification_result.update({
                        'parent_list_page_url': list_page['url'],
                        'parent_prefecture': list_page.get('prefecture', ''),
                        'parent_city': list_page.get('city', ''),
                        'subsidy_title_from_list': title,
                        'extraction_order': j,
                        'extracted_from_list': True
                    })

                    all_extracted_results.append(classification_result)
                    total_extracted_urls += 1

                    print(f"    ğŸ“ åˆ¤å®š: {classification_result.get('page_type', 'ä¸æ˜')} (ç¢ºä¿¡åº¦: {classification_result.get('confidence', 0.0):.2f})")

                    # APIè² è·è»½æ¸›ã®ãŸã‚å¾…æ©Ÿ
                    time.sleep(settings['extraction_delay'])

                except Exception as e:
                    print(f"    âŒ åˆ†é¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue

        # çµ±è¨ˆæƒ…å ±ã‚’ä½œæˆ
        statistics = create_extraction_statistics_from_found_subsidies(all_extracted_results, list_pages_with_subsidies)

        # çµæœã‚’ä¿å­˜
        base_filename = Path(classification_file).stem
        save_extraction_results_from_found_subsidies(all_extracted_results, statistics, base_filename)

        return {
            'success': True,
            'data': {
                'extracted_results': all_extracted_results,
                'statistics': statistics,
                'total_list_pages': len(list_pages_with_subsidies),
                'total_extracted_urls': total_extracted_urls
            },
            'total_extracted': total_extracted_urls
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

def create_extraction_statistics_from_found_subsidies(extracted_results, original_list_pages):
    """
    found_new_housing_subsidiesã‹ã‚‰ã®æŠ½å‡ºçµæœçµ±è¨ˆã‚’ä½œæˆ
    """
    stats = {
        'total_extracted': len(extracted_results),
        'by_page_type': {},
        'by_prefecture': {},
        'individual_pages_found': 0,
        'confidence_stats': {},
        'original_list_pages': len(original_list_pages)
    }

    # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
    for result in extracted_results:
        page_type = result.get('page_type', 'ä¸æ˜')
        stats['by_page_type'][page_type] = stats['by_page_type'].get(page_type, 0) + 1

    # éƒ½é“åºœçœŒåˆ¥çµ±è¨ˆ
    for result in extracted_results:
        pref = result.get('parent_prefecture', 'ä¸æ˜')
        stats['by_prefecture'][pref] = stats['by_prefecture'].get(pref, 0) + 1

    # å€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°ï¼ˆæ–°ç¯‰ä½å®…é–¢é€£å€‹åˆ¥ãƒšãƒ¼ã‚¸ï¼‰
    stats['individual_pages_found'] = stats['by_page_type'].get('æ–°ç¯‰ä½å®…é–¢é€£å€‹åˆ¥ãƒšãƒ¼ã‚¸', 0)

    # ç¢ºä¿¡åº¦çµ±è¨ˆ
    confidences = [r.get('confidence', 0.0) for r in extracted_results if r.get('confidence') is not None]
    if confidences:
        stats['confidence_stats'] = {
            'average': sum(confidences) / len(confidences),
            'max': max(confidences),
            'min': min(confidences)
        }

    return stats

def save_extraction_results_from_found_subsidies(extracted_results, statistics, base_filename):
    """
    found_new_housing_subsidiesã‹ã‚‰ã®æŠ½å‡ºçµæœã‚’ä¿å­˜
    """
    try:
        # ã™ã¹ã¦ã®çµæœã‚’ä¿å­˜
        all_results_file = f"{base_filename}_extracted_all.json"
        with open(all_results_file, 'w', encoding='utf-8') as f:
            json.dump(extracted_results, f, ensure_ascii=False, indent=2)
        print(f"âœ… å…¨æŠ½å‡ºçµæœä¿å­˜: {all_results_file}")

        # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®ã¿ã‚’æŠ½å‡º
        individual_pages = [r for r in extracted_results if r.get('page_type') == 'æ–°ç¯‰ä½å®…é–¢é€£å€‹åˆ¥ãƒšãƒ¼ã‚¸']

        if individual_pages:
            # å€‹åˆ¥ãƒšãƒ¼ã‚¸URLãƒªã‚¹ãƒˆ
            individual_urls_file = f"{base_filename}_extracted_individual_urls.txt"
            with open(individual_urls_file, 'w', encoding='utf-8') as f:
                for page in individual_pages:
                    f.write(f"{page.get('url', '')}\n")
            print(f"âœ… æŠ½å‡ºå€‹åˆ¥ãƒšãƒ¼ã‚¸URLãƒªã‚¹ãƒˆ: {individual_urls_file} ({len(individual_pages)}ä»¶)")

            # å€‹åˆ¥ãƒšãƒ¼ã‚¸è©³ç´°
            individual_detailed_file = f"{base_filename}_extracted_individual_detailed.json"
            with open(individual_detailed_file, 'w', encoding='utf-8') as f:
                json.dump(individual_pages, f, ensure_ascii=False, indent=2)
            print(f"âœ… æŠ½å‡ºå€‹åˆ¥ãƒšãƒ¼ã‚¸è©³ç´°: {individual_detailed_file}")

        # çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜
        stats_file = f"{base_filename}_extraction_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(statistics, f, ensure_ascii=False, indent=2)
        print(f"âœ… çµ±è¨ˆæƒ…å ±ä¿å­˜: {stats_file}")

        # çµ±è¨ˆè¡¨ç¤º
        print(f"\nğŸ“Š æŠ½å‡ºçµ±è¨ˆ:")
        print(f"  - å‡¦ç†å¯¾è±¡ä¸€è¦§ãƒšãƒ¼ã‚¸æ•°: {statistics['original_list_pages']}")
        print(f"  - ç·æŠ½å‡ºURLæ•°: {statistics['total_extracted']}")
        print(f"  - æ–°ç¯‰ä½å®…é–¢é€£å€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°: {statistics['individual_pages_found']}")

        if statistics.get('confidence_stats'):
            conf_stats = statistics['confidence_stats']
            print(f"  - å¹³å‡ç¢ºä¿¡åº¦: {conf_stats['average']:.2f}")

        print(f"\nğŸ“„ ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ¥:")
        for page_type, count in statistics['by_page_type'].items():
            print(f"  - {page_type}: {count}ä»¶")

    except Exception as e:
        print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")

def step4_merge_results(classification_file, extraction_data, base_filename, settings):
    """
    ã‚¹ãƒ†ãƒƒãƒ—4: çµæœãƒãƒ¼ã‚¸
    """
    try:
        # å…ƒã®åˆ†é¡çµæœã‚’èª­ã¿è¾¼ã¿
        original_results = load_classification_results(classification_file)
        extracted_results = extraction_data['extracted_results']

        # ãƒãƒ¼ã‚¸å®Ÿè¡Œ
        merged_data = merge_classification_results(original_results, extracted_results)

        # åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼ä½œæˆ
        comprehensive_summary = create_comprehensive_summary(merged_data)

        # çµæœã‚’ä¿å­˜
        save_merged_results(merged_data, comprehensive_summary, base_filename)

        final_url_file = f"{base_filename}_merged_individual_urls.txt"

        return {
            'success': True,
            'final_url_file': final_url_file,
            'total_individual_pages': merged_data['statistics']['merged_count'],
            'statistics': merged_data['statistics']
        }

    except Exception as e:
        return {
            'success': False,
            'error': str(e)
        }

def print_final_summary(results):
    """
    æœ€çµ‚ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
    """
    print(f"\n{'='*60}")
    print(f"ğŸ‰ {results['prefecture']} ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"{'='*60}")

    # å„ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœ
    steps = ['search', 'classification', 'extraction', 'merge']
    step_names = ['URLæ¤œç´¢', 'ãƒšãƒ¼ã‚¸åˆ†é¡', 'URLæŠ½å‡º', 'çµæœãƒãƒ¼ã‚¸']

    for step, name in zip(steps, step_names):
        step_result = results['step_results'].get(step, {})
        if step_result.get('success'):
            print(f"âœ… {name}: æˆåŠŸ")
        else:
            print(f"âŒ {name}: å¤±æ•—")

    # æœ€çµ‚çµ±è¨ˆ
    if results.get('success'):
        merge_stats = results['step_results']['merge']['statistics']
        print(f"\nğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
        print(f"  - åˆå›æ¤œç´¢ç”±æ¥ã®å€‹åˆ¥ãƒšãƒ¼ã‚¸: {merge_stats['original_count']}ä»¶")
        print(f"  - ä¸€è¦§ãƒšãƒ¼ã‚¸æŠ½å‡ºç”±æ¥: {merge_stats['extracted_count']}ä»¶")
        print(f"  - é‡è¤‡é™¤å»å¾Œã®ç·å€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°: {merge_stats['merged_count']}ä»¶")
        print(f"  - æ–°è¦ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°: {merge_stats['new_from_extraction']}ä»¶")

        print(f"\nğŸ’¾ æœ€çµ‚URLãƒªã‚¹ãƒˆ: {results['final_url_file']}")

def interactive_prefecture_processor():
    """
    å¯¾è©±çš„ãªéƒ½é“åºœçœŒå‡¦ç†
    """
    print("ğŸ›ï¸ éƒ½é“åºœçœŒåˆ¥è£œåŠ©é‡‘URLçµ±åˆå‡¦ç†ãƒ„ãƒ¼ãƒ«")
    print("=" * 50)

    # éƒ½é“åºœçœŒåã‚’å…¥åŠ›
    prefecture_name = input("å‡¦ç†ã™ã‚‹éƒ½é“åºœçœŒåã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ").strip()

    if not prefecture_name:
        print("âŒ éƒ½é“åºœçœŒåãŒå…¥åŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    # è¨­å®šã‚’ç¢ºèª
    print(f"\nâš™ï¸  å‡¦ç†è¨­å®š:")
    print(f"1. URLæŠ½å‡ºã«OpenAI APIã‚’ä½¿ç”¨ï¼ˆæ¨å¥¨ï¼‰")
    print(f"2. å‡¦ç†ã™ã‚‹å¸‚åŒºç”ºæ‘æ•°: å…¨ã¦")
    print(f"3. APIå‘¼ã³å‡ºã—é–“éš”: åˆ†é¡5ç§’ã€æŠ½å‡º5ç§’")

    use_custom = input("\nè¨­å®šã‚’å¤‰æ›´ã—ã¾ã™ã‹ï¼Ÿ (y/N): ").strip().lower()

    settings = {}
    if use_custom == 'y':
        try:
            max_cities = input("å‡¦ç†ã™ã‚‹æœ€å¤§å¸‚åŒºç”ºæ‘æ•° (ç©ºç™½=å…¨ã¦): ").strip()
            settings['max_cities'] = int(max_cities) if max_cities else None

            use_openai = input("URLæŠ½å‡ºã«OpenAI APIã‚’ä½¿ç”¨ã—ã¾ã™ã‹ï¼Ÿ (Y/n): ").strip().lower()
            settings['use_openai_for_extraction'] = use_openai != 'n'

            classification_delay = input("åˆ†é¡APIå‘¼ã³å‡ºã—é–“éš”ï¼ˆç§’ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5): ").strip()
            settings['classification_delay'] = int(classification_delay) if classification_delay else 5

            extraction_delay = input("æŠ½å‡ºAPIå‘¼ã³å‡ºã—é–“éš”ï¼ˆç§’ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5): ").strip()
            settings['extraction_delay'] = int(extraction_delay) if extraction_delay else 5

        except ValueError:
            print("âš ï¸  ç„¡åŠ¹ãªå…¥åŠ›ãŒã‚ã‚Šã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            settings = {}

    # ç¢ºèª
    print(f"\nğŸš€ {prefecture_name} ã®å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
    confirm = input("ã‚ˆã‚ã—ã„ã§ã™ã‹ï¼Ÿ (Y/n): ").strip().lower()

    if confirm == 'n':
        print("âŒ å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
        return

    # å‡¦ç†å®Ÿè¡Œ
    start_time = time.time()
    results = process_prefecture(prefecture_name, settings)
    end_time = time.time()

    # å‡¦ç†æ™‚é–“è¡¨ç¤º
    elapsed_time = end_time - start_time
    print(f"\nâ±ï¸  ç·å‡¦ç†æ™‚é–“: {elapsed_time/60:.1f}åˆ†")

    if results.get('success'):
        print(f"ğŸ‰ å‡¦ç†ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"ğŸ“ æœ€çµ‚çµæœ: {results['final_url_file']}")
    else:
        print(f"âŒ å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ: {results.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")

def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    try:
        if len(sys.argv) > 1:
            # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰éƒ½é“åºœçœŒåã‚’å–å¾—
            prefecture_name = sys.argv[1]
            results = process_prefecture(prefecture_name)
        else:
            # å¯¾è©±çš„ãƒ¢ãƒ¼ãƒ‰
            interactive_prefecture_processor()

    except KeyboardInterrupt:
        print("\nâš ï¸  å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")

if __name__ == '__main__':
    main()