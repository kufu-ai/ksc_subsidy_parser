import pandas as pd
import os
import json
from dotenv import load_dotenv
from langchain_tavily import TavilySearch
import time
from utils import get_output_path

# .envã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

CITY_CSV_PATH = "data/address/city.csv"
SITE_CSV_PATH = "data/address/site.csv"
TAVILY_API_KEY = os.getenv("TAVILY_API_KEY")

# ç”¨é€”ãƒ¯ãƒ¼ãƒ‰ã¨æ”¯æ´ãƒ¯ãƒ¼ãƒ‰
PURPOSE_WORDS = ["ä½å®…"]
SUPPORT_WORDS = ["è£œåŠ©é‡‘"]

# prefecture_idã‹ã‚‰éƒ½é“åºœçœŒåã¸ã®å¯¾å¿œè¾æ›¸
PREFECTURE_MAP = {
    1: "åŒ—æµ·é“",
    2: "é’æ£®çœŒ",
    3: "å²©æ‰‹çœŒ",
    4: "å®®åŸçœŒ",
    5: "ç§‹ç”°çœŒ",
    6: "å±±å½¢çœŒ",
    7: "ç¦å³¶çœŒ",
    8: "èŒ¨åŸçœŒ",
    9: "æ ƒæœ¨çœŒ",
    10: "ç¾¤é¦¬çœŒ",
    11: "åŸ¼ç‰çœŒ",
    12: "åƒè‘‰çœŒ",
    13: "æ±äº¬éƒ½",
    14: "ç¥å¥ˆå·çœŒ",
    15: "æ–°æ½ŸçœŒ",
    16: "å¯Œå±±çœŒ",
    17: "çŸ³å·çœŒ",
    18: "ç¦äº•çœŒ",
    19: "å±±æ¢¨çœŒ",
    20: "é•·é‡çœŒ",
    21: "å²é˜œçœŒ",
    22: "é™å²¡çœŒ",
    23: "æ„›çŸ¥çœŒ",
    24: "ä¸‰é‡çœŒ",
    25: "æ»‹è³€çœŒ",
    26: "äº¬éƒ½åºœ",
    27: "å¤§é˜ªåºœ",
    28: "å…µåº«çœŒ",
    29: "å¥ˆè‰¯çœŒ",
    30: "å’Œæ­Œå±±çœŒ",
    31: "é³¥å–çœŒ",
    32: "å³¶æ ¹çœŒ",
    33: "å²¡å±±çœŒ",
    34: "åºƒå³¶çœŒ",
    35: "å±±å£çœŒ",
    36: "å¾³å³¶çœŒ",
    37: "é¦™å·çœŒ",
    38: "æ„›åª›çœŒ",
    39: "é«˜çŸ¥çœŒ",
    40: "ç¦å²¡çœŒ",
    41: "ä½è³€çœŒ",
    42: "é•·å´çœŒ",
    43: "ç†Šæœ¬çœŒ",
    44: "å¤§åˆ†çœŒ",
    45: "å®®å´çœŒ",
    46: "é¹¿å…å³¶çœŒ",
    47: "æ²–ç¸„çœŒ",
}


def get_cities_by_prefecture(prefecture_name: str, city_csv_path: str = CITY_CSV_PATH):
    """
    æŒ‡å®šã—ãŸéƒ½é“åºœçœŒåã«å±ã™ã‚‹å¸‚åŒºç”ºæ‘åãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼ˆæ­£å¼åç§°ã®ã¿ï¼‰
    """
    df = pd.read_csv(city_csv_path)

    # prefecture_idã‚’éƒ½é“åºœçœŒåã‹ã‚‰é€†å¼•ã
    prefecture_id = None
    for pid, pname in PREFECTURE_MAP.items():
        if pname == prefecture_name:
            prefecture_id = pid
            break

    if prefecture_id is None:
        raise ValueError(f'éƒ½é“åºœçœŒå "{prefecture_name}" ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')

    # æŒ‡å®šã—ãŸéƒ½é“åºœçœŒã®å¸‚åŒºç”ºæ‘åã‚’å–å¾—ï¼ˆé‡è¤‡é™¤å»ï¼‰
    cities = df[df["prefecture_id"] == prefecture_id]["city_name"].unique().tolist()
    return cities


def get_flexible_city_name(
    input_city: str, prefecture_name: str, city_csv_path: str = CITY_CSV_PATH
):
    """
    å…¥åŠ›ã•ã‚ŒãŸå¸‚åŒºç”ºæ‘åã«å¯¾ã—ã¦ã€æŸ”è»Ÿã«ãƒãƒƒãƒã™ã‚‹æ­£å¼åç§°ã‚’è¿”ã™

    Args:
        input_city (str): å…¥åŠ›ã•ã‚ŒãŸå¸‚åŒºç”ºæ‘å
        prefecture_name (str): éƒ½é“åºœçœŒå
        city_csv_path (str): CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        str: ãƒãƒƒãƒã—ãŸæ­£å¼ãªå¸‚åŒºç”ºæ‘åã€è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å…ƒã®åå‰
    """
    df = pd.read_csv(city_csv_path)

    # prefecture_idã‚’éƒ½é“åºœçœŒåã‹ã‚‰é€†å¼•ã
    prefecture_id = None
    for pid, pname in PREFECTURE_MAP.items():
        if pname == prefecture_name:
            prefecture_id = pid
            break

    if prefecture_id is None:
        return input_city

    # ãã®éƒ½é“åºœçœŒã®å…¨å¸‚åŒºç”ºæ‘ã‚’å–å¾—
    prefecture_cities = df[df["prefecture_id"] == prefecture_id]["city_name"].unique()

    # 1. å®Œå…¨ä¸€è‡´ã‚’ç¢ºèª
    if input_city in prefecture_cities:
        return input_city

    # 2. å¾Œæ–¹ä¸€è‡´ã‚’ç¢ºèªï¼ˆã€Œå·è¶Šç”ºã€ãŒã€Œä¸‰é‡éƒ¡å·è¶Šç”ºã€ã«ãƒãƒƒãƒï¼‰
    for city in prefecture_cities:
        if city.endswith(input_city):
            return city

    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…ƒã®åå‰ã‚’è¿”ã™
    return input_city


def get_official_domain(city: str, prefecture: str, site_csv_path=SITE_CSV_PATH):
    """
    å¸‚åŒºç”ºæ‘åã‹ã‚‰å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å–å¾—ï¼ˆæŸ”è»Ÿãƒãƒƒãƒãƒ³ã‚°å¯¾å¿œï¼‰

    Args:
        city (str): å¸‚åŒºç”ºæ‘åï¼ˆæ­£å¼åç§°ã€éƒ¡åä»˜ãå¯èƒ½æ€§ã‚ã‚Šï¼‰
        prefecture (str): éƒ½é“åºœçœŒå
        site_csv_path (str): site.csvã®ãƒ‘ã‚¹

    Returns:
        str: å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°Noneï¼‰
    """
    try:
        df = pd.read_csv(site_csv_path)

        # ãã®éƒ½é“åºœçœŒã®site.csvãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        prefecture_sites = df[df["pref"] == prefecture]

        # 1. å®Œå…¨ä¸€è‡´ã‚’ç¢ºèª
        exact_match = prefecture_sites[prefecture_sites["city"] == city]
        if not exact_match.empty and pd.notnull(exact_match.iloc[0]["url"]):
            url = exact_match.iloc[0]["url"]
            return url.replace("https://", "").replace("http://", "").split("/")[0]

        # 2. å¾Œæ–¹ä¸€è‡´ã‚’ç¢ºèªï¼ˆã€Œè¶Šæ™ºéƒ¡ä¸Šå³¶ç”ºã€ãŒã€Œä¸Šå³¶ç”ºã€ã«ãƒãƒƒãƒï¼‰
        for _, row in prefecture_sites.iterrows():
            site_city = row["city"]
            if (
                pd.notnull(site_city)
                and city.endswith(site_city)
                and pd.notnull(row["url"])
            ):
                url = row["url"]
                return url.replace("https://", "").replace("http://", "").split("/")[0]

        return None

    except Exception as e:
        print(f"ãƒ‰ãƒ¡ã‚¤ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None


def search_subsidy_urls_detailed(city: str, prefecture: str, max_results=20):
    """
    å¸‚åŒºç”ºæ‘åãƒ»éƒ½é“åºœçœŒåãƒ»ç”¨é€”ãƒ¯ãƒ¼ãƒ‰ãƒ»æ”¯æ´ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã¿åˆã‚ã›ã§Tavilyæ¤œç´¢ã—ã€
    ã‚¯ã‚¨ãƒªã”ã¨ã«è©³ç´°ãªãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ï¼ˆé‡è¤‡å‰Šé™¤ãªã—ï¼‰

    Args:
        city (str): å¸‚åŒºç”ºæ‘å
        prefecture (str): éƒ½é“åºœçœŒå
        max_results (int): æ¤œç´¢çµæœã®æœ€å¤§æ•°
        save_txt (bool): URLã‚’txtãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹

    Returns:
        dict: å¸‚åŒºç”ºæ‘åã‚’ã‚­ãƒ¼ã¨ã—ãŸè¾æ›¸
        "å¸‚åŒºç”ºæ‘å": [
            {
                "ã‚¯ã‚¨ãƒª": "æ¤œç´¢ã‚¯ã‚¨ãƒª",
                "URL": ["URL1", "URL2", ...],
                "URLæ•°": å–å¾—URLæ•°,
                "status": "success" | "error",
                "error_message": "ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚¸ï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã®ã¿ï¼‰"
            },
            ...
        ]...
    """
    # æŸ”è»Ÿãƒãƒƒãƒãƒ³ã‚°ã§æ­£å¼åç§°ã‚’å–å¾—
    formal_city_name = get_flexible_city_name(city, prefecture)
    if formal_city_name != city:
        print(f"    ğŸ“ æ­£å¼åç§°: {city} â†’ {formal_city_name}")

    # æ­£å¼åç§°ã§å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å–å¾—
    domain = get_official_domain(formal_city_name, prefecture)
    # è¤‡æ•°æŒ‡å®šã—ãŸã„ãŒã†ã¾ãå–å¾—ã§ããªã„ã®ã§æ¯”è¼ƒçš„å¤šã„pdfã®ã¿é™¤å¤–
    minus_query = "-filetype:pdf"

    # å¸‚åŒºç”ºæ‘åã‚’ã‚­ãƒ¼ã¨ã—ãŸè¾æ›¸ã‚’åˆæœŸåŒ–
    city_results = {city: []}

    for purpose in PURPOSE_WORDS:
        for support in SUPPORT_WORDS:
            if domain:
                print(f"    ğŸŒ å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ä½¿ç”¨: {domain}")
                query = f"{formal_city_name} {purpose} {support} {minus_query}"
            else:
                print(f"    ğŸ” å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³æœªç™ºè¦‹ã€ä¸€èˆ¬æ¤œç´¢ã‚’å®Ÿè¡Œ")
                query = f"{prefecture}{formal_city_name} {purpose} {support} å…¬å¼ {minus_query}"

            query_result = {"ã‚¯ã‚¨ãƒª": query, "URL": [], "URLæ•°": 0, "status": "success"}

            try:
                tavily = TavilySearch(
                    api_key=TAVILY_API_KEY,
                    max_results=max_results,
                    include_domains=[domain],
                )

                print(f"    ğŸ” query: {query}")
                results = tavily.invoke({"query": query})
                urls = []
                for r in results.get("results", []):
                    url = r.get("url")
                    if url:
                        urls.append(url)

                query_result["URL"] = urls
                query_result["URLæ•°"] = len(urls)
                time.sleep(1.0)  # APIè² è·è»½æ¸›ã®ãŸã‚

            except Exception as e:
                print(f"    âŒ æ¤œç´¢å¤±æ•—: {query} ({e})")
                query_result["status"] = "error"
                query_result["error_message"] = str(e)

            city_results[city].append(query_result)

    return city_results


def search_subsidy_urls_detailed_prefecture(
    prefecture: str, max_results=20, save_files=True, limit_cities=None
):
    """
    éƒ½é“åºœçœŒå…¨ä½“ã®å¸‚åŒºç”ºæ‘ã§è£œåŠ©é‡‘URLã‚’æ¤œç´¢ã—ã€ã¾ã¨ã‚ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    Args:
        prefecture (str): éƒ½é“åºœçœŒå
        max_results (int): å„æ¤œç´¢ã‚¯ã‚¨ãƒªã®æœ€å¤§çµæœæ•°
        save_files (bool): ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
        limit_cities (int): å‡¦ç†ã™ã‚‹å¸‚åŒºç”ºæ‘æ•°ã®ä¸Šé™ï¼ˆãƒ†ã‚¹ãƒˆç”¨ã€Noneã§å…¨ã¦ï¼‰

    Returns:
        dict: å…¨å¸‚åŒºç”ºæ‘ã®æ¤œç´¢çµæœã‚’å«ã‚€è¾æ›¸
        {
            "å¸‚åŒºç”ºæ‘å1": [
                {
                    "ã‚¯ã‚¨ãƒª": "æ¤œç´¢ã‚¯ã‚¨ãƒª",
                    "URL": ["URL1", "URL2", ...],
                    "URLæ•°": å–å¾—URLæ•°,
                    "status": "success" | "error",
                    "error_message": "ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆã‚¨ãƒ©ãƒ¼æ™‚ã®ã¿ï¼‰"
                },
                ...
            ],
            "å¸‚åŒºç”ºæ‘å2": [...],
            ...
        }
    """
    print(f"ğŸŒ {prefecture}ã®è£œåŠ©é‡‘URLæ¤œç´¢ã‚’é–‹å§‹")

    # éƒ½é“åºœçœŒå†…ã®å…¨å¸‚åŒºç”ºæ‘ã‚’å–å¾—
    cities = get_cities_by_prefecture(prefecture)
    print(f"ğŸ“ å¯¾è±¡å¸‚åŒºç”ºæ‘æ•°: {len(cities)}ä»¶")

    # å‡¦ç†ã™ã‚‹å¸‚åŒºç”ºæ‘æ•°ã‚’åˆ¶é™ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    if limit_cities:
        cities = cities[:limit_cities]
        print(f"âš ï¸  ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰: {limit_cities}ä»¶ã®ã¿å‡¦ç†")

    all_results = {}
    all_urls = []

    for i, city in enumerate(cities, 1):
        print(f"\nğŸ“ ({i}/{len(cities)}) {city} ã‚’å‡¦ç†ä¸­...")

        try:
            # å„å¸‚åŒºç”ºæ‘ã®è©³ç´°æ¤œç´¢ã‚’å®Ÿè¡Œ
            city_result = search_subsidy_urls_detailed(city, prefecture, max_results)
            all_results.update(city_result)

            # æˆåŠŸã—ãŸURLã‚’åé›†
            for query_data in city_result[city]:
                if query_data["status"] == "success":
                    all_urls.extend(query_data["URL"])

            print(f"âœ… {city}: å®Œäº†")
            # break  # TODO ä¸è¦ã«ãªã£ãŸã‚‰æ¶ˆã™

        except Exception as e:
            print(f"âŒ {city}: ã‚¨ãƒ©ãƒ¼ - {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå¸‚åŒºç”ºæ‘ã‚‚è¨˜éŒ²
            all_results[city] = [
                {
                    "ã‚¯ã‚¨ãƒª": "",
                    "URL": [],
                    "URLæ•°": 0,
                    "status": "error",
                    "error_message": f"å¸‚åŒºç”ºæ‘å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}",
                }
            ]

    print(f"\nğŸ“Š æ¤œç´¢å®Œäº†:")
    print(f"  å‡¦ç†å¸‚åŒºç”ºæ‘æ•°: {len(all_results)}")
    print(f"  ç·URLæ•°: {len(all_urls)}")

    # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
    if save_files:
        # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        safe_prefecture = prefecture.replace("/", "_").replace("\\", "_")

        # 1. è©³ç´°JSONãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        json_filename = get_output_path(f"{safe_prefecture}_subsidy_urls_detailed.json")
        try:
            with open(json_filename, "w", encoding="utf-8") as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
            print(f"ğŸ’¾ è©³ç´°ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {json_filename}")
        except Exception as e:
            print(f"âŒ JSONä¿å­˜å¤±æ•—: {e}")

        # 2. URLãƒªã‚¹ãƒˆtxtãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        txt_filename = get_output_path(f"{safe_prefecture}_all_urls.txt")
        try:
            with open(txt_filename, "w", encoding="utf-8") as f:
                for url in all_urls:
                    f.write(f"{url}\n")
            print(f"ğŸ’¾ URLãƒªã‚¹ãƒˆä¿å­˜: {txt_filename} ({len(all_urls)}ä»¶)")
        except Exception as e:
            print(f"âŒ txtä¿å­˜å¤±æ•—: {e}")

        # 4. æ¤œç´¢çµæœè©³ç´°CSVä¿å­˜
        csv_detailed_filename = get_output_path(
            f"{safe_prefecture}_search_results_detailed.csv"
        )
        try:
            csv_data = []

            for city, data in all_results.items():
                for query_info in data:
                    # å„URLã‚’å€‹åˆ¥ã®è¡Œã¨ã—ã¦è¨˜éŒ²ï¼ˆURLãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
                    if query_info["URLæ•°"] > 0:
                        for url in query_info["URL"]:
                            csv_data.append(
                                {
                                    "éƒ½é“åºœçœŒ": prefecture,
                                    "å¸‚åŒºç”ºæ‘": city,
                                    "ã‚¯ã‚¨ãƒª": query_info["ã‚¯ã‚¨ãƒª"],
                                    "URL": url,
                                }
                            )

            df_detailed = pd.DataFrame(csv_data)
            df_detailed.to_csv(csv_detailed_filename, index=False, encoding="utf-8-sig")
            print(
                f"ğŸ“Š æ¤œç´¢çµæœè©³ç´°CSVä¿å­˜: {csv_detailed_filename} ({len(csv_data)}è¡Œ)"
            )
        except Exception as e:
            print(f"âŒ è©³ç´°CSVä¿å­˜å¤±æ•—: {e}")

    return all_results


def main():
    prefecture = input("éƒ½é“åºœçœŒåã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ")
    cities = get_cities_by_prefecture(prefecture)
    print(f"{prefecture}ã®å¸‚åŒºç”ºæ‘æ•°: {len(cities)}")
    search_subsidy_urls_detailed_prefecture(prefecture)
    return prefecture


if __name__ == "__main__":
    main()
