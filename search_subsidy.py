import pandas as pd
import os
from dotenv import load_dotenv
from langchain_tavily import TavilySearch
import time

# .envã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

CITY_CSV_PATH = 'data/address/city.csv'
SITE_CSV_PATH = 'data/address/site.csv'
TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')

# ç”¨é€”ãƒ¯ãƒ¼ãƒ‰ã¨æ”¯æ´ãƒ¯ãƒ¼ãƒ‰
PURPOSE_WORDS = ["ä½å®…", "åœŸåœ°"]
SUPPORT_WORDS = ["è£œåŠ©é‡‘"]

# prefecture_idã‹ã‚‰éƒ½é“åºœçœŒåã¸ã®å¯¾å¿œè¾æ›¸
PREFECTURE_MAP = {
    1: 'åŒ—æµ·é“', 2: 'é’æ£®çœŒ', 3: 'å²©æ‰‹çœŒ', 4: 'å®®åŸçœŒ', 5: 'ç§‹ç”°çœŒ', 6: 'å±±å½¢çœŒ', 7: 'ç¦å³¶çœŒ',
    8: 'èŒ¨åŸçœŒ', 9: 'æ ƒæœ¨çœŒ', 10: 'ç¾¤é¦¬çœŒ', 11: 'åŸ¼ç‰çœŒ', 12: 'åƒè‘‰çœŒ', 13: 'æ±äº¬éƒ½', 14: 'ç¥å¥ˆå·çœŒ',
    15: 'æ–°æ½ŸçœŒ', 16: 'å¯Œå±±çœŒ', 17: 'çŸ³å·çœŒ', 18: 'ç¦äº•çœŒ', 19: 'å±±æ¢¨çœŒ', 20: 'é•·é‡çœŒ', 21: 'å²é˜œçœŒ',
    22: 'é™å²¡çœŒ', 23: 'æ„›çŸ¥çœŒ', 24: 'ä¸‰é‡çœŒ', 25: 'æ»‹è³€çœŒ', 26: 'äº¬éƒ½åºœ', 27: 'å¤§é˜ªåºœ', 28: 'å…µåº«çœŒ',
    29: 'å¥ˆè‰¯çœŒ', 30: 'å’Œæ­Œå±±çœŒ', 31: 'é³¥å–çœŒ', 32: 'å³¶æ ¹çœŒ', 33: 'å²¡å±±çœŒ', 34: 'åºƒå³¶çœŒ', 35: 'å±±å£çœŒ',
    36: 'å¾³å³¶çœŒ', 37: 'é¦™å·çœŒ', 38: 'æ„›åª›çœŒ', 39: 'é«˜çŸ¥çœŒ', 40: 'ç¦å²¡çœŒ', 41: 'ä½è³€çœŒ', 42: 'é•·å´çœŒ',
    43: 'ç†Šæœ¬çœŒ', 44: 'å¤§åˆ†çœŒ', 45: 'å®®å´çœŒ', 46: 'é¹¿å…å³¶çœŒ', 47: 'æ²–ç¸„çœŒ'
}

def get_cities_by_prefecture(prefecture_name: str, city_csv_path: str = CITY_CSV_PATH):
    """
    æŒ‡å®šã—ãŸéƒ½é“åºœçœŒåã«å±ã™ã‚‹å¸‚åŒºç”ºæ‘åãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼ˆæ­£å¼åç§°ã®ã¿ï¼‰
    """
    df = pd.read_csv(city_csv_path)

    # prefecture_idã‚’éƒ½é“åºœçœŒåã‹ã‚‰é€†å¼•ã
    prefecture_id = None
    for pid, pname in PREFECTURE_MAP.items():
        if pname == prefecture_name:
            prefecture_id = pid
            break

    if prefecture_id is None:
        raise ValueError(f'éƒ½é“åºœçœŒå "{prefecture_name}" ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')

    # æŒ‡å®šã—ãŸéƒ½é“åºœçœŒã®å¸‚åŒºç”ºæ‘åã‚’å–å¾—ï¼ˆé‡è¤‡é™¤å»ï¼‰
    cities = df[df['prefecture_id'] == prefecture_id]['city_name'].unique().tolist()
    return cities

def get_flexible_city_name(input_city: str, prefecture_name: str, city_csv_path: str = CITY_CSV_PATH):
    """
    å…¥åŠ›ã•ã‚ŒãŸå¸‚åŒºç”ºæ‘åã«å¯¾ã—ã¦ã€æŸ”è»Ÿã«ãƒãƒƒãƒã™ã‚‹æ­£å¼åç§°ã‚’è¿”ã™

    Args:
        input_city (str): å…¥åŠ›ã•ã‚ŒãŸå¸‚åŒºç”ºæ‘å
        prefecture_name (str): éƒ½é“åºœçœŒå
        city_csv_path (str): CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        str: ãƒãƒƒãƒã—ãŸæ­£å¼ãªå¸‚åŒºç”ºæ‘åã€è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å…ƒã®åå‰
    """
    df = pd.read_csv(city_csv_path)

    # prefecture_idã‚’éƒ½é“åºœçœŒåã‹ã‚‰é€†å¼•ã
    prefecture_id = None
    for pid, pname in PREFECTURE_MAP.items():
        if pname == prefecture_name:
            prefecture_id = pid
            break

    if prefecture_id is None:
        return input_city

    # ãã®éƒ½é“åºœçœŒã®å…¨å¸‚åŒºç”ºæ‘ã‚’å–å¾—
    prefecture_cities = df[df['prefecture_id'] == prefecture_id]['city_name'].unique()

    # 1. å®Œå…¨ä¸€è‡´ã‚’ç¢ºèª
    if input_city in prefecture_cities:
        return input_city

    # 2. å¾Œæ–¹ä¸€è‡´ã‚’ç¢ºèªï¼ˆã€Œå·è¶Šç”ºã€ãŒã€Œä¸‰é‡éƒ¡å·è¶Šç”ºã€ã«ãƒãƒƒãƒï¼‰
    for city in prefecture_cities:
        if city.endswith(input_city):
            return city

    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…ƒã®åå‰ã‚’è¿”ã™
    return input_city

def get_official_domain(city: str, prefecture: str, site_csv_path=SITE_CSV_PATH):
    """
    å¸‚åŒºç”ºæ‘åã‹ã‚‰å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å–å¾—ï¼ˆæŸ”è»Ÿãƒãƒƒãƒãƒ³ã‚°å¯¾å¿œï¼‰

    Args:
        city (str): å¸‚åŒºç”ºæ‘åï¼ˆæ­£å¼åç§°ã€éƒ¡åä»˜ãå¯èƒ½æ€§ã‚ã‚Šï¼‰
        prefecture (str): éƒ½é“åºœçœŒå
        site_csv_path (str): site.csvã®ãƒ‘ã‚¹

    Returns:
        str: å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°Noneï¼‰
    """
    try:
        df = pd.read_csv(site_csv_path)

        # ãã®éƒ½é“åºœçœŒã®site.csvãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        prefecture_sites = df[df['pref'] == prefecture]

        # 1. å®Œå…¨ä¸€è‡´ã‚’ç¢ºèª
        exact_match = prefecture_sites[prefecture_sites['city'] == city]
        if not exact_match.empty and pd.notnull(exact_match.iloc[0]['url']):
            url = exact_match.iloc[0]['url']
            return url.replace('https://', '').replace('http://', '').split('/')[0]

        # 2. å¾Œæ–¹ä¸€è‡´ã‚’ç¢ºèªï¼ˆã€Œè¶Šæ™ºéƒ¡ä¸Šå³¶ç”ºã€ãŒã€Œä¸Šå³¶ç”ºã€ã«ãƒãƒƒãƒï¼‰
        for _, row in prefecture_sites.iterrows():
            site_city = row['city']
            if pd.notnull(site_city) and city.endswith(site_city) and pd.notnull(row['url']):
                url = row['url']
                return url.replace('https://', '').replace('http://', '').split('/')[0]

        return None

    except Exception as e:
        print(f"ãƒ‰ãƒ¡ã‚¤ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def search_subsidy_urls(city: str, prefecture: str, max_results=10):
    """
    å¸‚åŒºç”ºæ‘åãƒ»éƒ½é“åºœçœŒåãƒ»ç”¨é€”ãƒ¯ãƒ¼ãƒ‰ãƒ»æ”¯æ´ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã¿åˆã‚ã›ã§Tavilyæ¤œç´¢ã—ã€URLãƒªã‚¹ãƒˆã‚’è¿”ã™
    å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒã‚ã‚Œã°site:ã§çµã‚Šè¾¼ã‚€ï¼ˆget_flexible_city_nameã‚’ä½¿ç”¨ï¼‰
    """
    tavily = TavilySearch(api_key=TAVILY_API_KEY, max_results=max_results)
    urls = set()

    # æŸ”è»Ÿãƒãƒƒãƒãƒ³ã‚°ã§æ­£å¼åç§°ã‚’å–å¾—
    formal_city_name = get_flexible_city_name(city, prefecture)
    if formal_city_name != city:
        print(f"    ğŸ“ æ­£å¼åç§°: {city} â†’ {formal_city_name}")

    # æ­£å¼åç§°ã§å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å–å¾—
    domain = get_official_domain(formal_city_name, prefecture)
    # è¤‡æ•°æŒ‡å®šã—ãŸã„ãŒã†ã¾ãå–å¾—ã§ããªã„ã®ã§æ¯”è¼ƒçš„å¤šã„pdfã®ã¿é™¤å¤–
    minus_query = '-filetype:pdf'

    for purpose in PURPOSE_WORDS:
        for support in SUPPORT_WORDS:
            if domain:
                print(f"    ğŸŒ å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ä½¿ç”¨: {domain}")
                query = f"{purpose} {support} site:{domain} {minus_query}"
            else:
                print(f"    ğŸ” å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³æœªç™ºè¦‹ã€ä¸€èˆ¬æ¤œç´¢ã‚’å®Ÿè¡Œ")
                query = f"{prefecture} {formal_city_name} {purpose} {support} å…¬å¼ {minus_query}"
            try:
                results = tavily.invoke({"query": query})
                for r in results.get('results', []):
                    url = r.get('url')
                    if url:
                        urls.add(url)
                time.sleep(1.0)  # APIè² è·è»½æ¸›ã®ãŸã‚
            except Exception as e:
                print(f"    âŒ æ¤œç´¢å¤±æ•—: {query} ({e})")
    return list(urls)


def main():
    prefecture = input('éƒ½é“åºœçœŒåã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ')
    cities = get_cities_by_prefecture(prefecture)
    print(f"{prefecture}ã®å¸‚åŒºç”ºæ‘æ•°: {len(cities)}")
    result_list = []
    for city in cities:
        urls = search_subsidy_urls(city, prefecture)
        result_list.append({"éƒ½é“åºœçœŒå": prefecture, "city_name": city, "è£œåŠ©é‡‘é–¢é€£URL": urls})
        print(f"{city}: {len(urls)}ä»¶ã®URLã‚’å–å¾—")
        # TODO: æœ€å¾Œãƒ»é€šã—ã®ãƒã‚§ãƒƒã‚¯ã®æ™‚ã¯æ¶ˆã™
        # 2ä»¶å–å¾—ã—ãŸã‚‰çµ‚äº† ãƒªãƒŸãƒƒãƒˆæ¥ãªã„ã‚ˆã†ã«
        if len(result_list) >= 2:
            break
    # çµæœã‚’CSV/JSONã§ä¿å­˜
    df_result = pd.DataFrame(result_list)
    df_result.to_json(f"{prefecture}_subsidy_urls.json", force_ascii=False, orient="records", indent=2)
    # df_result.to_csv(f"{prefecture}_subsidy_urls.csv", index=False)
    print(f"ä¿å­˜å®Œäº†: {prefecture}_subsidy_urls.json, {prefecture}_subsidy_urls.csv")

if __name__ == '__main__':
    main()
