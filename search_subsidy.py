import pandas as pd
import os
from dotenv import load_dotenv
from langchain_tavily import TavilySearch
import time

# .envã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

CITY_CSV_PATH = 'data/address/city.csv'
SITE_CSV_PATH = 'data/address/site.csv'
TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')

# ç”¨é€”ãƒ¯ãƒ¼ãƒ‰ã¨æ”¯æ´ãƒ¯ãƒ¼ãƒ‰
PURPOSE_WORDS = ["ä½å®…", "åœŸåœ°"]
SUPPORT_WORDS = ["è£œåŠ©é‡‘"]

def get_cities_by_prefecture(prefecture_name: str, city_csv_path: str = CITY_CSV_PATH):
    """
    æŒ‡å®šã—ãŸéƒ½é“åºœçœŒåã«å±ã™ã‚‹å¸‚åŒºç”ºæ‘åãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼ˆæ­£å¼åç§°ã®ã¿ï¼‰
    """
    df = pd.read_csv(city_csv_path)
    # éƒ½é“åºœçœŒåã®ã‚«ãƒ©ãƒ åã¯csvã®å†…å®¹ã«åˆã‚ã›ã¦ä¿®æ­£ã—ã¦ãã ã•ã„
    # ä¾‹: 'éƒ½é“åºœçœŒå', 'å¸‚åŒºç”ºæ‘å'
    if 'éƒ½é“åºœçœŒå' not in df.columns or 'å¸‚åŒºç”ºæ‘å' not in df.columns:
        raise ValueError('city.csvã®ã‚«ãƒ©ãƒ åãŒæƒ³å®šã¨ç•°ãªã‚Šã¾ã™')

    # æ­£å¼åç§°ã®ã¿ã‚’è¿”ã™ï¼ˆé‡è¤‡é™¤å»ï¼‰
    cities = df[df['éƒ½é“åºœçœŒå'] == prefecture_name]['å¸‚åŒºç”ºæ‘å'].unique().tolist()
    return cities

def get_flexible_city_name(input_city: str, prefecture_name: str, city_csv_path: str = CITY_CSV_PATH):
    """
    å…¥åŠ›ã•ã‚ŒãŸå¸‚åŒºç”ºæ‘åã«å¯¾ã—ã¦ã€æŸ”è»Ÿã«ãƒãƒƒãƒã™ã‚‹æ­£å¼åç§°ã‚’è¿”ã™

    Args:
        input_city (str): å…¥åŠ›ã•ã‚ŒãŸå¸‚åŒºç”ºæ‘å
        prefecture_name (str): éƒ½é“åºœçœŒå
        city_csv_path (str): CSVãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

    Returns:
        str: ãƒãƒƒãƒã—ãŸæ­£å¼ãªå¸‚åŒºç”ºæ‘åã€è¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°å…ƒã®åå‰
    """
    df = pd.read_csv(city_csv_path)

    # ãã®éƒ½é“åºœçœŒã®å…¨å¸‚åŒºç”ºæ‘ã‚’å–å¾—
    prefecture_cities = df[df['éƒ½é“åºœçœŒå'] == prefecture_name]['å¸‚åŒºç”ºæ‘å'].unique()

    # 1. å®Œå…¨ä¸€è‡´ã‚’ç¢ºèª
    if input_city in prefecture_cities:
        return input_city

    # 2. å¾Œæ–¹ä¸€è‡´ã‚’ç¢ºèªï¼ˆã€Œå·è¶Šç”ºã€ãŒã€Œä¸‰é‡éƒ¡å·è¶Šç”ºã€ã«ãƒãƒƒãƒï¼‰
    for city in prefecture_cities:
        if city.endswith(input_city):
            return city

    # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å…ƒã®åå‰ã‚’è¿”ã™
    return input_city

def get_official_domain(city: str, prefecture: str, site_csv_path=SITE_CSV_PATH):
    """
    å¸‚åŒºç”ºæ‘åã‹ã‚‰å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å–å¾—ï¼ˆæŸ”è»Ÿãƒãƒƒãƒãƒ³ã‚°å¯¾å¿œï¼‰

    Args:
        city (str): å¸‚åŒºç”ºæ‘åï¼ˆæ­£å¼åç§°ã€éƒ¡åä»˜ãå¯èƒ½æ€§ã‚ã‚Šï¼‰
        prefecture (str): éƒ½é“åºœçœŒå
        site_csv_path (str): site.csvã®ãƒ‘ã‚¹

    Returns:
        str: å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆè¦‹ã¤ã‹ã‚‰ãªã‘ã‚Œã°Noneï¼‰
    """
    try:
        df = pd.read_csv(site_csv_path)

        # ãã®éƒ½é“åºœçœŒã®site.csvãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        prefecture_sites = df[df['pref'] == prefecture]

        # 1. å®Œå…¨ä¸€è‡´ã‚’ç¢ºèª
        exact_match = prefecture_sites[prefecture_sites['city'] == city]
        if not exact_match.empty and pd.notnull(exact_match.iloc[0]['url']):
            url = exact_match.iloc[0]['url']
            return url.replace('https://', '').replace('http://', '').split('/')[0]

        # 2. å¾Œæ–¹ä¸€è‡´ã‚’ç¢ºèªï¼ˆã€Œè¶Šæ™ºéƒ¡ä¸Šå³¶ç”ºã€ãŒã€Œä¸Šå³¶ç”ºã€ã«ãƒãƒƒãƒï¼‰
        for _, row in prefecture_sites.iterrows():
            site_city = row['city']
            if pd.notnull(site_city) and city.endswith(site_city) and pd.notnull(row['url']):
                url = row['url']
                return url.replace('https://', '').replace('http://', '').split('/')[0]

        return None

    except Exception as e:
        print(f"ãƒ‰ãƒ¡ã‚¤ãƒ³å–å¾—ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

def search_subsidy_urls(city: str, prefecture: str, max_results=20):
    """
    å¸‚åŒºç”ºæ‘åãƒ»éƒ½é“åºœçœŒåãƒ»ç”¨é€”ãƒ¯ãƒ¼ãƒ‰ãƒ»æ”¯æ´ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã¿åˆã‚ã›ã§Tavilyæ¤œç´¢ã—ã€URLãƒªã‚¹ãƒˆã‚’è¿”ã™
    å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ãŒã‚ã‚Œã°site:ã§çµã‚Šè¾¼ã‚€ï¼ˆget_flexible_city_nameã‚’ä½¿ç”¨ï¼‰
    """
    tavily = TavilySearch(api_key=TAVILY_API_KEY)
    urls = set()

    # æŸ”è»Ÿãƒãƒƒãƒãƒ³ã‚°ã§æ­£å¼åç§°ã‚’å–å¾—
    formal_city_name = get_flexible_city_name(city, prefecture)
    if formal_city_name != city:
        print(f"    ğŸ“ æ­£å¼åç§°: {city} â†’ {formal_city_name}")

    # æ­£å¼åç§°ã§å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’å–å¾—
    domain = get_official_domain(formal_city_name, prefecture)

    for purpose in PURPOSE_WORDS:
        for support in SUPPORT_WORDS:
            if domain:
                print(f"    ğŸŒ å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’ä½¿ç”¨: {domain}")
                query = f"{prefecture} {formal_city_name} {purpose} {support} site:{domain}"
            else:
                print(f"    ğŸ” å…¬å¼ãƒ‰ãƒ¡ã‚¤ãƒ³æœªç™ºè¦‹ã€ä¸€èˆ¬æ¤œç´¢ã‚’å®Ÿè¡Œ")
                query = f"{prefecture} {formal_city_name} {purpose} {support} å…¬å¼"
            try:
                results = tavily.invoke({"query": query, "max_results": max_results})
                for r in results.get('results', []):
                    url = r.get('url')
                    if url:
                        urls.add(url)
                time.sleep(1.0)  # APIè² è·è»½æ¸›ã®ãŸã‚
            except Exception as e:
                print(f"    âŒ æ¤œç´¢å¤±æ•—: {query} ({e})")
    return list(urls)


def main():
    prefecture = input('éƒ½é“åºœçœŒåã‚’å…¥åŠ›ã—ã¦ãã ã•ã„: ')
    cities = get_cities_by_prefecture(prefecture)
    print(f"{prefecture}ã®å¸‚åŒºç”ºæ‘æ•°: {len(cities)}")
    result_list = []
    for city in cities:
        urls = search_subsidy_urls(city, prefecture)
        result_list.append({"éƒ½é“åºœçœŒå": prefecture, "å¸‚åŒºç”ºæ‘å": city, "è£œåŠ©é‡‘é–¢é€£URL": urls})
        print(f"{city}: {len(urls)}ä»¶ã®URLã‚’å–å¾—")
        # TODO: æœ€å¾Œãƒ»é€šã—ã®ãƒã‚§ãƒƒã‚¯ã®æ™‚ã¯æ¶ˆã™
        # 2ä»¶å–å¾—ã—ãŸã‚‰çµ‚äº† ãƒªãƒŸãƒƒãƒˆæ¥ãªã„ã‚ˆã†ã«
        # if len(result_list) >= 2:
        #     break
    # çµæœã‚’CSV/JSONã§ä¿å­˜
    df_result = pd.DataFrame(result_list)
    df_result.to_json(f"{prefecture}_subsidy_urls.json", force_ascii=False, orient="records", indent=2)
    df_result.to_csv(f"{prefecture}_subsidy_urls.csv", index=False)
    print(f"ä¿å­˜å®Œäº†: {prefecture}_subsidy_urls.json, {prefecture}_subsidy_urls.csv")

if __name__ == '__main__':
    main()
