#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
OpenAI APIã‚’ä½¿ç”¨ã—ãŸé«˜ç²¾åº¦URLæŠ½å‡ºãƒ„ãƒ¼ãƒ«
"""

import pandas as pd
import json
import os
import time
import re
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from html_fetcher import fetch_html
from page_classifier import classify_page_type
from openai_handler import client
from pathlib import Path

# OpenAI APIã‚’ä½¿ã£ãŸURLæŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
URL_EXTRACTION_PROMPT = """
ã‚ãªãŸã¯è‡ªæ²»ä½“ã®è£œåŠ©é‡‘æƒ…å ±ã‚µã‚¤ãƒˆã®å°‚é–€å®¶ã§ã™ã€‚

ä»¥ä¸‹ã®HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’åˆ†æã—ã€è£œåŠ©é‡‘åˆ¶åº¦ã«é–¢é€£ã™ã‚‹ãƒªãƒ³ã‚¯ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

**æŠ½å‡ºå¯¾è±¡ã®ãƒªãƒ³ã‚¯ï¼š**
- å…·ä½“çš„ãªè£œåŠ©é‡‘åˆ¶åº¦ã®è©³ç´°ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯
- è£œåŠ©é‡‘ã®ç”³è«‹ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯
- è£œåŠ©é‡‘åˆ¶åº¦ã®ä¸€è¦§ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯
- å„ç¨®æ”¯æ´åˆ¶åº¦ã®èª¬æ˜ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯

**é™¤å¤–ã™ã¹ããƒªãƒ³ã‚¯ï¼š**
- ãƒŠãƒ“ã‚²ãƒ¼ã‚·ãƒ§ãƒ³ãƒãƒ¼ã®ãƒªãƒ³ã‚¯ï¼ˆã€Œãƒ›ãƒ¼ãƒ ã€ã€ŒãŠçŸ¥ã‚‰ã›ã€ã€Œçµ„ç¹”æ¡ˆå†…ã€ãªã©ï¼‰
- ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®ä¸€èˆ¬çš„ãªãƒªãƒ³ã‚¯
- ãƒ•ãƒƒã‚¿ãƒ¼ã®ãƒªãƒ³ã‚¯ï¼ˆã€Œãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ãƒãƒªã‚·ãƒ¼ã€ã€Œã‚µã‚¤ãƒˆãƒãƒƒãƒ—ã€ãªã©ï¼‰
- å¤–éƒ¨ã‚µã‚¤ãƒˆã¸ã®ãƒªãƒ³ã‚¯ï¼ˆSNSã€ä»–è‡ªæ²»ä½“ãªã©ï¼‰
- ãƒ•ã‚¡ã‚¤ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ï¼ˆPDFã€Wordç­‰ï¼‰
- ä¸€èˆ¬çš„ãªè¡Œæ”¿ã‚µãƒ¼ãƒ“ã‚¹ã®ãƒªãƒ³ã‚¯ï¼ˆæˆ¸ç±ã€ç¨å‹™ãªã©ï¼‰

**å‡ºåŠ›å½¢å¼ï¼š**
ä»¥ä¸‹ã®JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„ï¼š
{
  "subsidy_related_urls": [
    {
      "url": "æŠ½å‡ºã—ãŸURL",
      "link_text": "ãƒªãƒ³ã‚¯ã®ãƒ†ã‚­ã‚¹ãƒˆ",
      "relevance_score": 0.0-1.0ã®é–¢é€£åº¦ã‚¹ã‚³ã‚¢,
      "reasoning": "æŠ½å‡ºç†ç”±ï¼ˆæ—¥æœ¬èª50æ–‡å­—ä»¥å†…ï¼‰"
    }
  ],
  "page_analysis": {
    "total_links_found": å…¨ãƒªãƒ³ã‚¯æ•°,
    "subsidy_links_extracted": æŠ½å‡ºã—ãŸãƒªãƒ³ã‚¯æ•°,
    "main_content_area_identified": "ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ã‚’ç‰¹å®šã§ããŸã‹ã©ã†ã‹"
  }
}

é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã¯ä»¥ä¸‹ã®åŸºæº–ã§è¨­å®šã—ã¦ãã ã•ã„ï¼š
- 0.9-1.0: æ˜ç¢ºã«ç‰¹å®šã®è£œåŠ©é‡‘åˆ¶åº¦ã¸ã®ç›´æ¥ãƒªãƒ³ã‚¯
- 0.7-0.8: è£œåŠ©é‡‘ä¸€è¦§ã‚„æ”¯æ´åˆ¶åº¦ã®ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸
- 0.5-0.6: é–¢é€£ã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ãƒšãƒ¼ã‚¸ï¼ˆè¦ç¢ºèªï¼‰
- 0.0-0.4: é–¢é€£æ€§ãŒä½ã„ï¼ˆé€šå¸¸ã¯é™¤å¤–ï¼‰

0.5ä»¥ä¸Šã®ã‚¹ã‚³ã‚¢ã®ãƒªãƒ³ã‚¯ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
"""

def extract_urls_with_openai(html_content, base_url, max_content_length=40000):
    """
    OpenAI APIã‚’ä½¿ã£ã¦URLã‚’æŠ½å‡º

    Args:
        html_content (str): HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        base_url (str): ãƒ™ãƒ¼ã‚¹URL
        max_content_length (int): æœ€å¤§ã‚³ãƒ³ãƒ†ãƒ³ãƒ„é•·

    Returns:
        dict: æŠ½å‡ºçµæœ
    """
    try:
        # HTMLãŒé•·ã™ãã‚‹å ´åˆã¯åˆ¶é™
        if len(html_content) > max_content_length:
            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å„ªå…ˆçš„ã«å–å¾—
            soup = BeautifulSoup(html_content, 'html.parser')

            # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ã‚’ç‰¹å®š
            main_content = None
            for selector in ['main', '[role="main"]', '.main-content', '.content', '#content', '.main']:
                main_element = soup.select_one(selector)
                if main_element:
                    main_content = str(main_element)
                    break

            if main_content:
                html_content = main_content[:max_content_length]
            else:
                html_content = html_content[:max_content_length]

            html_content += "\n...(ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒé•·ã„ãŸã‚çœç•¥ã•ã‚Œã¾ã—ãŸ)"

        # OpenAI APIã§URLæŠ½å‡º
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": URL_EXTRACTION_PROMPT
                },
                {
                    "role": "user",
                    "content": f"ãƒ™ãƒ¼ã‚¹URL: {base_url}\n\nä»¥ä¸‹ã®HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰è£œåŠ©é‡‘é–¢é€£URLã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼š\n\n{html_content}"
                }
            ],
            response_format={
                "type": "json_schema",
                "json_schema": {
                    "name": "url_extraction",
                    "strict": True,
                    "schema": {
                        "type": "object",
                        "properties": {
                            "subsidy_related_urls": {
                                "type": "array",
                                "items": {
                                    "type": "object",
                                    "properties": {
                                        "url": {"type": "string"},
                                        "link_text": {"type": "string"},
                                        "relevance_score": {"type": "number", "minimum": 0.0, "maximum": 1.0},
                                        "reasoning": {"type": "string"}
                                    },
                                    "required": ["url", "link_text", "relevance_score", "reasoning"],
                                    "additionalProperties": False
                                }
                            },
                            "page_analysis": {
                                "type": "object",
                                "properties": {
                                    "total_links_found": {"type": "integer"},
                                    "subsidy_links_extracted": {"type": "integer"},
                                    "main_content_area_identified": {"type": "string"}
                                },
                                "required": ["total_links_found", "subsidy_links_extracted", "main_content_area_identified"],
                                "additionalProperties": False
                            }
                        },
                        "required": ["subsidy_related_urls", "page_analysis"],
                        "additionalProperties": False
                    }
                }
            },
            max_tokens=2000,
            temperature=0.1
        )

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ‘ãƒ¼ã‚¹
        response_content = response.choices[0].message.content
        result = json.loads(response_content)

        # URLã‚’çµ¶å¯¾URLã«å¤‰æ›
        processed_urls = []
        for url_info in result['subsidy_related_urls']:
            original_url = url_info['url']
            absolute_url = urljoin(base_url, original_url)

            url_info['url'] = absolute_url
            url_info['original_url'] = original_url
            processed_urls.append(url_info)

        result['subsidy_related_urls'] = processed_urls
        return result

    except Exception as e:
        print(f"OpenAI URLæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            'subsidy_related_urls': [],
            'page_analysis': {
                'total_links_found': 0,
                'subsidy_links_extracted': 0,
                'main_content_area_identified': 'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ'
            },
            'error': str(e)
        }

def extract_urls_with_beautifulsoup_improved(html_content, base_url):
    """
    æ”¹è‰¯ã•ã‚ŒãŸBeautifulSoupã«ã‚ˆã‚‹URLæŠ½å‡ºï¼ˆæ¯”è¼ƒç”¨ï¼‰

    Args:
        html_content (str): HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        base_url (str): ãƒ™ãƒ¼ã‚¹URL

    Returns:
        dict: æŠ½å‡ºçµæœ
    """
    try:
        soup = BeautifulSoup(html_content, 'html.parser')

        # é™¤å¤–ã™ã¹ãè¦ç´ ã‚’ç‰¹å®š
        exclude_selectors = [
            'nav', '.nav', '#nav', '.navigation',
            'header', '.header', '#header',
            'footer', '.footer', '#footer',
            '.sidebar', '#sidebar', '.side',
            '.breadcrumb', '.breadcrumbs',
            '.menu', '.global-menu',
            '[role="navigation"]',
            '.sns', '.social'
        ]

        # é™¤å¤–è¦ç´ ã‚’å‰Šé™¤
        for selector in exclude_selectors:
            for element in soup.select(selector):
                element.decompose()

        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ã‚’ç‰¹å®š
        main_content = None
        for selector in ['main', '[role="main"]', '.main-content', '.content', '#content', '.main']:
            main_element = soup.select_one(selector)
            if main_element:
                main_content = main_element
                break

        # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¨ãƒªã‚¢ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯bodyå…¨ä½“ã‚’ä½¿ç”¨
        if main_content is None:
            main_content = soup.find('body') or soup

        # ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        links = main_content.find_all('a', href=True)

        # è£œåŠ©é‡‘é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        include_keywords = [
            'è£œåŠ©', 'åŠ©æˆ', 'æ”¯æ´', 'äº¤ä»˜', 'çµ¦ä»˜', 'å¥¨åŠ±',
            'åˆ¶åº¦', 'äº‹æ¥­', 'ç”³è«‹', 'å‹Ÿé›†', 'å¯¾è±¡',
            'subsidy', 'grant', 'support', 'aid'
        ]

        exclude_keywords = [
            'javascript:', 'mailto:', 'tel:', '#',
            '.pdf', '.doc', '.xls', '.zip', '.csv',
            'facebook', 'twitter', 'instagram', 'youtube',
            'login', 'admin', 'search', 'sitemap',
            'privacy', 'contact', 'about', 'access'
        ]

        extracted_urls = []

        for link in links:
            href = link.get('href', '')
            text = link.get_text(strip=True)

            # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            if any(keyword in href.lower() for keyword in exclude_keywords):
                continue

            # ç©ºã®ãƒªãƒ³ã‚¯ã‚„ã‚¢ãƒ³ã‚«ãƒ¼ã®ã¿ã¯é™¤å¤–
            if not href or href == '#':
                continue

            # çµ¶å¯¾URLã«å¤‰æ›
            absolute_url = urljoin(base_url, href)

            # é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
            relevance_score = 0.0
            reasoning = ""

            # ãƒ†ã‚­ã‚¹ãƒˆã¨URLã§é–¢é€£æ€§ã‚’åˆ¤å®š
            combined_text = (text + " " + href).lower()
            matching_keywords = [kw for kw in include_keywords if kw in combined_text]

            if matching_keywords:
                relevance_score = min(0.7, 0.4 + len(matching_keywords) * 0.1)
                reasoning = f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è‡´: {', '.join(matching_keywords[:2])}"
            elif any(word in combined_text for word in ['åˆ¶åº¦', 'äº‹æ¥­', 'ç”³è«‹', 'å‹Ÿé›†']):
                relevance_score = 0.5
                reasoning = "é–¢é€£ã™ã‚‹å¯èƒ½æ€§ã‚ã‚Š"

            if relevance_score >= 0.5:
                extracted_urls.append({
                    'url': absolute_url,
                    'link_text': text,
                    'relevance_score': relevance_score,
                    'reasoning': reasoning
                })

        return {
            'subsidy_related_urls': extracted_urls,
            'page_analysis': {
                'total_links_found': len(links),
                'subsidy_links_extracted': len(extracted_urls),
                'main_content_area_identified': 'BeautifulSoupæ”¹è‰¯ç‰ˆã§å‡¦ç†'
            }
        }

    except Exception as e:
        print(f"BeautifulSoupæ”¹è‰¯ç‰ˆã‚¨ãƒ©ãƒ¼: {str(e)}")
        return {
            'subsidy_related_urls': [],
            'page_analysis': {
                'total_links_found': 0,
                'subsidy_links_extracted': 0,
                'main_content_area_identified': 'ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ'
            },
            'error': str(e)
        }

def compare_extraction_methods(html_content, base_url):
    """
    OpenAIã¨BeautifulSoupã®æŠ½å‡ºçµæœã‚’æ¯”è¼ƒ

    Args:
        html_content (str): HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„
        base_url (str): ãƒ™ãƒ¼ã‚¹URL

    Returns:
        dict: æ¯”è¼ƒçµæœ
    """
    print("  ğŸ¤– OpenAI APIã§æŠ½å‡ºä¸­...")
    openai_result = extract_urls_with_openai(html_content, base_url)

    print("  ğŸ” BeautifulSoupæ”¹è‰¯ç‰ˆã§æŠ½å‡ºä¸­...")
    bs_result = extract_urls_with_beautifulsoup_improved(html_content, base_url)

    # çµæœã‚’æ¯”è¼ƒ
    openai_urls = set(url['url'] for url in openai_result['subsidy_related_urls'])
    bs_urls = set(url['url'] for url in bs_result['subsidy_related_urls'])

    comparison = {
        'openai_count': len(openai_urls),
        'beautifulsoup_count': len(bs_urls),
        'common_urls': list(openai_urls & bs_urls),
        'openai_only': list(openai_urls - bs_urls),
        'beautifulsoup_only': list(bs_urls - openai_urls),
        'openai_result': openai_result,
        'beautifulsoup_result': bs_result
    }

    return comparison

def smart_extract_and_classify_from_list_pages(classification_results, extraction_method="openai", max_urls_per_page=30, delay=3):
    """
    é«˜ç²¾åº¦æŠ½å‡ºã‚’ä½¿ã£ã¦ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰URLã‚’æŠ½å‡ºãƒ»åˆ†é¡

    Args:
        classification_results (list): åˆ†é¡çµæœ
        extraction_method (str): "openai", "beautifulsoup", "compare"
        max_urls_per_page (int): 1ãƒšãƒ¼ã‚¸ã‚ãŸã‚Šã®æœ€å¤§URLæ•°
        delay (int): APIå‘¼ã³å‡ºã—é–“ã®å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰

    Returns:
        dict: æŠ½å‡ºãƒ»åˆ†é¡çµæœ
    """
    # ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚’æŠ½å‡º
    list_pages = [r for r in classification_results if r.get('page_type') == 'ä¸€è¦§ãƒšãƒ¼ã‚¸']

    if not list_pages:
        print("âŒ ä¸€è¦§ãƒšãƒ¼ã‚¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return {'extracted_results': [], 'statistics': {}}

    print(f"ğŸ“‹ ä¸€è¦§ãƒšãƒ¼ã‚¸æ•°: {len(list_pages)}")
    print(f"ğŸ”§ æŠ½å‡ºæ–¹æ³•: {extraction_method}")

    all_extracted_results = []
    extraction_details = []
    total_extracted_urls = 0

    for i, list_page in enumerate(list_pages, 1):
        print(f"\n{'='*60}")
        print(f"ğŸ“„ ä¸€è¦§ãƒšãƒ¼ã‚¸ {i}/{len(list_pages)}: {list_page.get('url', '')}")
        print(f"ğŸ›ï¸ {list_page.get('prefecture', '')} {list_page.get('city', '')}")
        print(f"{'='*60}")

        try:
            # HTMLã‚’å–å¾—
            filename = f"smart_list_page_{int(time.time())}_{i}.html"
            html_path = fetch_html(list_page['url'], filename)

            # HTMLã‚’èª­ã¿è¾¼ã¿
            with open(html_path, 'r', encoding='utf-8') as f:
                html_content = f.read()

            # æŠ½å‡ºæ–¹æ³•ã«å¿œã˜ã¦å‡¦ç†
            if extraction_method == "openai":
                extraction_result = extract_urls_with_openai(html_content, list_page['url'])
                filtered_urls = [url['url'] for url in extraction_result['subsidy_related_urls']]
            elif extraction_method == "beautifulsoup":
                extraction_result = extract_urls_with_beautifulsoup_improved(html_content, list_page['url'])
                filtered_urls = [url['url'] for url in extraction_result['subsidy_related_urls']]
            elif extraction_method == "compare":
                comparison = compare_extraction_methods(html_content, list_page['url'])
                extraction_result = comparison
                # OpenAIçµæœã‚’å„ªå…ˆä½¿ç”¨
                filtered_urls = [url['url'] for url in comparison['openai_result']['subsidy_related_urls']]

            print(f"âœ… æŠ½å‡ºã•ã‚ŒãŸURLæ•°: {len(filtered_urls)}")

            # ä¸Šé™ã‚’é©ç”¨
            if len(filtered_urls) > max_urls_per_page:
                filtered_urls = filtered_urls[:max_urls_per_page]
                print(f"âš ï¸  ä¸Šé™é©ç”¨: {max_urls_per_page}ä»¶ã«åˆ¶é™")

            total_extracted_urls += len(filtered_urls)

            # æŠ½å‡ºè©³ç´°ã‚’ä¿å­˜
            extraction_details.append({
                'list_page_url': list_page['url'],
                'extraction_method': extraction_method,
                'extraction_result': extraction_result
            })

            # å„URLã‚’åˆ†é¡
            for j, url in enumerate(filtered_urls, 1):
                print(f"  ğŸ” {j}/{len(filtered_urls)}: {url[:80]}...")

                try:
                    # URLåˆ†é¡
                    classification_result = classify_page_type(url)

                    # å…ƒã®ä¸€è¦§ãƒšãƒ¼ã‚¸æƒ…å ±ã‚’è¿½åŠ 
                    classification_result.update({
                        'parent_list_page_url': list_page['url'],
                        'parent_prefecture': list_page.get('prefecture', ''),
                        'parent_city': list_page.get('city', ''),
                        'extraction_order': j,
                        'extracted_from_list': True,
                        'extraction_method': extraction_method
                    })

                    all_extracted_results.append(classification_result)

                    print(f"    ğŸ“ åˆ¤å®š: {classification_result.get('page_type', 'ä¸æ˜')} (ç¢ºä¿¡åº¦: {classification_result.get('confidence', 0.0):.2f})")

                    # APIè² è·è»½æ¸›ã®ãŸã‚å¾…æ©Ÿ
                    time.sleep(delay)

                except Exception as e:
                    print(f"    âŒ åˆ†é¡ã‚¨ãƒ©ãƒ¼: {str(e)}")
                    continue

            # HTMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            os.remove(html_path)

        except Exception as e:
            print(f"âŒ ä¸€è¦§ãƒšãƒ¼ã‚¸å‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)}")
            continue

    # çµ±è¨ˆæƒ…å ±ã‚’ä½œæˆ
    statistics = create_smart_extraction_statistics(all_extracted_results, list_pages, extraction_details)

    return {
        'extracted_results': all_extracted_results,
        'extraction_details': extraction_details,
        'statistics': statistics,
        'total_list_pages': len(list_pages),
        'total_extracted_urls': total_extracted_urls
    }

def create_smart_extraction_statistics(extracted_results, original_list_pages, extraction_details):
    """
    é«˜ç²¾åº¦æŠ½å‡ºã®çµ±è¨ˆã‚’ä½œæˆ
    """
    stats = {
        'total_extracted': len(extracted_results),
        'by_page_type': {},
        'by_prefecture': {},
        'individual_pages_found': 0,
        'confidence_stats': {},
        'extraction_method_stats': {}
    }

    # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ¥çµ±è¨ˆ
    for result in extracted_results:
        page_type = result.get('page_type', 'ä¸æ˜')
        stats['by_page_type'][page_type] = stats['by_page_type'].get(page_type, 0) + 1

    # éƒ½é“åºœçœŒåˆ¥çµ±è¨ˆ
    for result in extracted_results:
        pref = result.get('parent_prefecture', 'ä¸æ˜')
        stats['by_prefecture'][pref] = stats['by_prefecture'].get(pref, 0) + 1

    # å€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°
    stats['individual_pages_found'] = stats['by_page_type'].get('å€‹åˆ¥ãƒšãƒ¼ã‚¸', 0)

    # ç¢ºä¿¡åº¦çµ±è¨ˆ
    confidences = [r.get('confidence', 0.0) for r in extracted_results if r.get('confidence') is not None]
    if confidences:
        stats['confidence_stats'] = {
            'average': sum(confidences) / len(confidences),
            'max': max(confidences),
            'min': min(confidences)
        }

    # æŠ½å‡ºæ–¹æ³•åˆ¥çµ±è¨ˆ
    for detail in extraction_details:
        result = detail['extraction_result']
        if 'page_analysis' in result:
            analysis = result['page_analysis']
            stats['extraction_method_stats'][detail['list_page_url']] = {
                'total_links': analysis.get('total_links_found', 0),
                'extracted_links': analysis.get('subsidy_links_extracted', 0),
                'extraction_rate': analysis.get('subsidy_links_extracted', 0) / max(analysis.get('total_links_found', 1), 1)
            }

    return stats

def save_smart_extraction_results(results_data, base_filename):
    """
    é«˜ç²¾åº¦æŠ½å‡ºçµæœã‚’ä¿å­˜
    """
    try:
        extracted_results = results_data['extracted_results']
        statistics = results_data['statistics']
        extraction_details = results_data['extraction_details']

        # ã™ã¹ã¦ã®çµæœã‚’ä¿å­˜
        all_results_file = f"{base_filename}_smart_extracted_all.json"
        with open(all_results_file, 'w', encoding='utf-8') as f:
            json.dump(extracted_results, f, ensure_ascii=False, indent=2)
        print(f"âœ… å…¨æŠ½å‡ºçµæœä¿å­˜: {all_results_file}")

        # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®ã¿ã‚’æŠ½å‡º
        individual_pages = [r for r in extracted_results if r.get('page_type') == 'å€‹åˆ¥ãƒšãƒ¼ã‚¸']

        if individual_pages:
            # å€‹åˆ¥ãƒšãƒ¼ã‚¸URLãƒªã‚¹ãƒˆ
            individual_urls_file = f"{base_filename}_smart_individual_urls.txt"
            with open(individual_urls_file, 'w', encoding='utf-8') as f:
                for page in individual_pages:
                    f.write(f"{page.get('url', '')}\n")
            print(f"âœ… é«˜ç²¾åº¦å€‹åˆ¥ãƒšãƒ¼ã‚¸URLãƒªã‚¹ãƒˆ: {individual_urls_file} ({len(individual_pages)}ä»¶)")

            # å€‹åˆ¥ãƒšãƒ¼ã‚¸è©³ç´°
            individual_detailed_file = f"{base_filename}_smart_individual_detailed.json"
            with open(individual_detailed_file, 'w', encoding='utf-8') as f:
                json.dump(individual_pages, f, ensure_ascii=False, indent=2)
            print(f"âœ… é«˜ç²¾åº¦å€‹åˆ¥ãƒšãƒ¼ã‚¸è©³ç´°: {individual_detailed_file}")

        # æŠ½å‡ºè©³ç´°æƒ…å ±
        extraction_details_file = f"{base_filename}_smart_extraction_details.json"
        with open(extraction_details_file, 'w', encoding='utf-8') as f:
            json.dump(extraction_details, f, ensure_ascii=False, indent=2)
        print(f"âœ… æŠ½å‡ºè©³ç´°æƒ…å ±: {extraction_details_file}")

        # çµ±è¨ˆæƒ…å ±ã‚’ä¿å­˜
        stats_file = f"{base_filename}_smart_extraction_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(statistics, f, ensure_ascii=False, indent=2)
        print(f"âœ… çµ±è¨ˆæƒ…å ±ä¿å­˜: {stats_file}")

        # çµ±è¨ˆè¡¨ç¤º
        print_smart_extraction_statistics(statistics, results_data)

    except Exception as e:
        print(f"âŒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {str(e)}")

def print_smart_extraction_statistics(statistics, results_data):
    """
    é«˜ç²¾åº¦æŠ½å‡ºçµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š é«˜ç²¾åº¦URLæŠ½å‡ºçµæœçµ±è¨ˆ")
    print(f"{'='*60}")

    print(f"å‡¦ç†ã—ãŸä¸€è¦§ãƒšãƒ¼ã‚¸æ•°: {results_data['total_list_pages']}")
    print(f"æŠ½å‡ºãƒ»åˆ†é¡ã—ãŸURLç·æ•°: {statistics['total_extracted']}")
    print(f"æ–°ãŸã«ç™ºè¦‹ã—ãŸå€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°: {statistics['individual_pages_found']}")

    print(f"\n--- ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ¥ ---")
    for page_type, count in statistics['by_page_type'].items():
        print(f"{page_type}: {count}ä»¶")

    print(f"\n--- éƒ½é“åºœçœŒåˆ¥ ---")
    sorted_prefs = sorted(statistics['by_prefecture'].items(), key=lambda x: x[1], reverse=True)
    for pref, count in sorted_prefs[:10]:  # ä¸Šä½10éƒ½é“åºœçœŒ
        print(f"{pref}: {count}ä»¶")

    if statistics['confidence_stats']:
        conf_stats = statistics['confidence_stats']
        print(f"\n--- ç¢ºä¿¡åº¦çµ±è¨ˆ ---")
        print(f"å¹³å‡: {conf_stats['average']:.3f}")
        print(f"æœ€é«˜: {conf_stats['max']:.3f}")
        print(f"æœ€ä½: {conf_stats['min']:.3f}")

    # æŠ½å‡ºåŠ¹ç‡çµ±è¨ˆ
    extraction_stats = statistics['extraction_method_stats']
    if extraction_stats:
        total_links = sum(stat['total_links'] for stat in extraction_stats.values())
        total_extracted = sum(stat['extracted_links'] for stat in extraction_stats.values())
        avg_extraction_rate = sum(stat['extraction_rate'] for stat in extraction_stats.values()) / len(extraction_stats)

        print(f"\n--- æŠ½å‡ºåŠ¹ç‡çµ±è¨ˆ ---")
        print(f"ç·ãƒªãƒ³ã‚¯æ•°: {total_links}")
        print(f"æŠ½å‡ºãƒªãƒ³ã‚¯æ•°: {total_extracted}")
        print(f"å¹³å‡æŠ½å‡ºç‡: {avg_extraction_rate:.3f}")

def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    print("ğŸ§  é«˜ç²¾åº¦URLæŠ½å‡ºãƒ»åˆ†é¡ãƒ„ãƒ¼ãƒ«")
    print("-" * 50)

    # åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    classification_files = []
    for pattern in ['*_page_classification.json', '*_page_classification.csv']:
        classification_files.extend(Path('.').glob(pattern))

    if not classification_files:
        print("âŒ åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    print("åˆ©ç”¨å¯èƒ½ãªåˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«:")
    for i, file in enumerate(classification_files, 1):
        print(f"{i}. {file.name}")

    try:
        choice = int(input("\nå‡¦ç†ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã®ç•ªå·ã‚’é¸æŠã—ã¦ãã ã•ã„: ")) - 1
        if not (0 <= choice < len(classification_files)):
            print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
            return

        selected_file = classification_files[choice]

        # åˆ†é¡çµæœã‚’èª­ã¿è¾¼ã¿
        if selected_file.suffix == '.json':
            with open(selected_file, 'r', encoding='utf-8') as f:
                classification_results = json.load(f)
        else:
            df = pd.read_csv(selected_file)
            classification_results = df.to_dict('records')

        if not classification_results:
            print("âŒ åˆ†é¡çµæœãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
            return

        # æŠ½å‡ºæ–¹æ³•ã‚’é¸æŠ
        print("\næŠ½å‡ºæ–¹æ³•ã‚’é¸æŠã—ã¦ãã ã•ã„:")
        print("1. OpenAI APIï¼ˆé«˜ç²¾åº¦ãƒ»æ¨å¥¨ï¼‰")
        print("2. BeautifulSoupæ”¹è‰¯ç‰ˆï¼ˆé«˜é€Ÿãƒ»ç„¡æ–™ï¼‰")
        print("3. æ¯”è¼ƒãƒ¢ãƒ¼ãƒ‰ï¼ˆä¸¡æ–¹å®Ÿè¡Œã—ã¦æ¯”è¼ƒï¼‰")

        method_choice = input("é¸æŠ (1-3): ").strip()
        method_map = {"1": "openai", "2": "beautifulsoup", "3": "compare"}
        extraction_method = method_map.get(method_choice, "openai")

        # è¨­å®š
        max_urls = int(input("1ã¤ã®ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡ºã™ã‚‹æœ€å¤§URLæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 30): ") or "30")
        delay = int(input("APIå‘¼ã³å‡ºã—é–“éš”ï¼ˆç§’ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 3): ") or "3")

        print(f"\nğŸš€ å‡¦ç†é–‹å§‹...")
        print(f"   - æŠ½å‡ºæ–¹æ³•: {extraction_method}")
        print(f"   - æœ€å¤§URLæ•°/ãƒšãƒ¼ã‚¸: {max_urls}")
        print(f"   - APIå‘¼ã³å‡ºã—é–“éš”: {delay}ç§’")

        # é«˜ç²¾åº¦æŠ½å‡ºãƒ»åˆ†é¡å®Ÿè¡Œ
        results_data = smart_extract_and_classify_from_list_pages(
            classification_results,
            extraction_method=extraction_method,
            max_urls_per_page=max_urls,
            delay=delay
        )

        # çµæœã‚’ä¿å­˜
        base_filename = f"{selected_file.stem}_{extraction_method}"
        save_smart_extraction_results(results_data, base_filename)

        print(f"\nğŸ‰ å‡¦ç†å®Œäº†ï¼")

    except ValueError:
        print("âŒ ç„¡åŠ¹ãªå…¥åŠ›ã§ã™")
    except KeyboardInterrupt:
        print("\nâš ï¸  å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")

if __name__ == '__main__':
    main()
