import search_subsidy
import page_classifier
import json
import os
import merge_classification_results
import pandas as pd


def main():
    prefecture = search_subsidy.main()
    page_classifier.main(prefecture)

    # page_classification.jsonã®ä¸­ã§page_typeãŒè£œåŠ©é‡‘æƒ…å ±ä¸€è¦§ãƒšãƒ¼ã‚¸ã®found_new_housing_subsidiesã‚’å–å¾—ã™ã‚‹
    created_finalize_file = False
    with open(f"{prefecture}_page_classification.json", "r") as f:
        research_urls = {}
        data = json.load(f)
        for item in data:
            if item["page_type"] == "è£œåŠ©é‡‘æƒ…å ±ä¸€è¦§ãƒšãƒ¼ã‚¸":
                urls_list = []
                for subsidy in item["found_new_housing_subsidies"]:
                    urls_list.append(subsidy["url"])

                if urls_list:
                    research_urls[item["city"]] = [
                        {
                            "ã‚¯ã‚¨ãƒª": "ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—",
                            "URL": urls_list,
                            "URLæ•°": len(urls_list),
                            "status": "success",
                        }
                    ]
        if research_urls:
            # {prefecture}2_search_subsidy_urls_detailed.jsonã«research_urlsã‚’è¿½åŠ ã™ã‚‹
            with open(f"{prefecture}2_subsidy_urls_detailed.json", "w") as f:
                json.dump(research_urls, f, indent=2, ensure_ascii=False)

            ignore_already_find = research_urls.copy()
            # ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ã—ãŸURLã‚’å†åº¦page_classifierã§åˆ†é¡ã™ã‚‹
            # æ¤œç´¢ã§ç™ºè¦‹æ¸ˆã¿ã®URLã‚’ãƒã‚§ãƒƒã‚¯(*_all_urls.txt)ã—ã¦ã€åŒã˜URLãŒã‚ã‚‹å ´åˆã¯ignore_already_findã‹ã‚‰é™¤å¤–ã™ã‚‹
            with open(f"{prefecture}_all_urls.txt", "r") as f:
                all_urls = [url.strip() for url in f.readlines()]

            # ignore_already_findã®å„å¸‚åŒºç”ºæ‘ã®URLé…åˆ—ã‹ã‚‰ã€all_urlsã«å«ã¾ã‚Œã‚‹URLã‚’å‰Šé™¤ã™ã‚‹
            for city in ignore_already_find:
                for query_data in ignore_already_find[city]:
                    # URLé…åˆ—ã‹ã‚‰æ—¢å­˜URLã‚’å‰Šé™¤
                    original_urls = query_data["URL"]
                    filtered_urls = [
                        url for url in original_urls if url not in all_urls
                    ]
                    query_data["URL"] = filtered_urls
                    query_data["URLæ•°"] = len(filtered_urls)

            # ç©ºã®URLé…åˆ—ã‚’æŒã¤å¸‚åŒºç”ºæ‘ã¯å‰Šé™¤
            ignore_already_find = {
                city: data
                for city, data in ignore_already_find.items()
                if any(query_data["URL"] for query_data in data)
            }

            classification_research = []
            if ignore_already_find:
                print(f"ğŸ˜® æ¤œç´¢ã§ç™ºè¦‹ã—ã¦ã„ãªã„URLãŒã‚ã‚Šã¾ã™ã€‚")
                # æ¤œç´¢ã§ç™ºè¦‹ã—ã¦ã„ãªã„URLã‚’page_classifierã§åˆ†é¡ã™ã‚‹
                print(f"ä¸€è¦§ãƒšãƒ¼ã‚¸ã§ç™ºè¦‹ã—ãŸURLã‚’åˆ†é¡ã—ã¾ã™...")
                classification_research = page_classifier.classify_urls_from_object(
                    ignore_already_find, prefecture
                )

            print(f"âœ… å…¨ã¦ã®URLã‚’åˆ†é¡ã—ã¾ã—ãŸã€‚")

            # çµæœã‚’ãƒãƒ¼ã‚¸ã™ã‚‹
            if classification_research:
                print(f"åˆ†é¡çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¾ã™...")
                merged_data = merge_classification_results.merge_classification_results(
                    data,
                    classification_research,
                )

                # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                with open(f"{prefecture}_all_classification.json", "w") as f:
                    json.dump(merged_data, f, indent=2, ensure_ascii=False)
                print(f"âœ… çµ±åˆJSON: {prefecture}_all_classification.json")

                # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä½œæˆ
                df_merged = pd.DataFrame(merged_data)
                csv_file = f"{prefecture}_all_classification.csv"
                df_merged.to_csv(csv_file, index=False, encoding="utf-8")
                print(f"âœ… çµ±åˆCSV: {csv_file}")

                created_finalize_file = True
    # ä½œæˆã•ã‚Œã¦ã„ãªã‹ã£ãŸã‚‰ã€æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ä½œæˆã™ã‚‹
    if not created_finalize_file:
        with open(f"{prefecture}_page_classification.json", "r") as f:
            data = json.load(f)
            # jsonã‚’ä½œã‚‹
            with open(f"{prefecture}_all_classification.json", "w") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"âœ… çµ±åˆJSON: {prefecture}_all_classification.json")

            # csvã‚’ä½œã‚‹
            pd_data = pd.DataFrame(data)
            csv_file = f"{prefecture}_all_classification.csv"
            pd_data.to_csv(csv_file, index=False, encoding="utf-8")
            print(f"âœ… çµ±åˆCSV: {csv_file}")


if __name__ == "__main__":
    main()
