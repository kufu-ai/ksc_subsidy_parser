import search_subsidy
import page_classifier
import json
import os
import merge_classification_results
import pandas as pd


def main():
    prefecture = search_subsidy.main()
    page_classifier.main(prefecture)

    # page_classification.jsonã®ä¸­ã§page_typeãŒè£œåŠ©é‡‘æƒ…å ±ä¸€è¦§ãƒšãƒ¼ã‚¸ã®found_new_housing_subsidiesã‚’å–å¾—ã™ã‚‹
    created_finalize_file = False
    with open(f"{prefecture}_page_classification.json", "r") as f:
        research_urls = {}
        data = json.load(f)
        for item in data:
            if item["page_type"] == "è£œåŠ©é‡‘æƒ…å ±ä¸€è¦§ãƒšãƒ¼ã‚¸":
                urls_list = []
                for subsidy in item["found_new_housing_subsidies"]:
                    urls_list.append(subsidy["url"])

                if urls_list:
                    research_urls[item["city"]] = [
                        {
                            "ã‚¯ã‚¨ãƒª": "ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—",
                            "URL": urls_list,
                            "URLæ•°": len(urls_list),
                            "status": "success",
                        }
                    ]
        if research_urls:
            # {prefecture}2_search_subsidy_urls_detailed.jsonã«research_urlsã‚’è¿½åŠ ã™ã‚‹
            with open(f"{prefecture}2_subsidy_urls_detailed.json", "w") as f:
                json.dump(research_urls, f, indent=2, ensure_ascii=False)

            ignore_already_find = research_urls.copy()
            # ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰å–å¾—ã—ãŸURLã‚’å†åº¦page_classifierã§åˆ†é¡ã™ã‚‹
            # æ¤œç´¢ã§ç™ºè¦‹æ¸ˆã¿ã®URLã‚’ãƒã‚§ãƒƒã‚¯(*_all_urls.txt)ã—ã¦ã€åŒã˜URLãŒã‚ã‚‹å ´åˆã¯ignore_already_findã‹ã‚‰é™¤å¤–ã™ã‚‹
            with open(f"{prefecture}_all_urls.txt", "r") as f:
                all_urls = [url.strip() for url in f.readlines()]

            # ignore_already_findã®å„å¸‚åŒºç”ºæ‘ã®URLé…åˆ—ã‹ã‚‰ã€all_urlsã«å«ã¾ã‚Œã‚‹URLã‚’å‰Šé™¤ã™ã‚‹
            for city in ignore_already_find:
                for query_data in ignore_already_find[city]:
                    # URLé…åˆ—ã‹ã‚‰æ—¢å­˜URLã‚’å‰Šé™¤
                    original_urls = query_data["URL"]
                    filtered_urls = [
                        url for url in original_urls if url not in all_urls
                    ]
                    query_data["URL"] = filtered_urls
                    query_data["URLæ•°"] = len(filtered_urls)

            # ç©ºã®URLé…åˆ—ã‚’æŒã¤å¸‚åŒºç”ºæ‘ã¯å‰Šé™¤
            ignore_already_find = {
                city: data
                for city, data in ignore_already_find.items()
                if any(query_data["URL"] for query_data in data)
            }

            classification_research = []
            if ignore_already_find:
                print(f"ğŸ˜® æ¤œç´¢ã§ç™ºè¦‹ã—ã¦ã„ãªã„URLãŒã‚ã‚Šã¾ã™ã€‚")
                # æ¤œç´¢ã§ç™ºè¦‹ã—ã¦ã„ãªã„URLã‚’page_classifierã§åˆ†é¡ã™ã‚‹
                print(f"ä¸€è¦§ãƒšãƒ¼ã‚¸ã§ç™ºè¦‹ã—ãŸURLã‚’åˆ†é¡ã—ã¾ã™...")
                classification_research = page_classifier.classify_urls_from_object(
                    ignore_already_find, prefecture
                )

            print(f"âœ… å…¨ã¦ã®URLã‚’åˆ†é¡ã—ã¾ã—ãŸã€‚")

            # çµæœã‚’ãƒãƒ¼ã‚¸ã™ã‚‹
            if classification_research:
                print(f"åˆ†é¡çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¾ã™...")
                merged_data = merge_classification_results.merge_classification_results(
                    data,
                    classification_research,
                )

                # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                with open(f"{prefecture}_all_classification.json", "w") as f:
                    json.dump(merged_data, f, indent=2, ensure_ascii=False)
                print(f"âœ… çµ±åˆJSON: {prefecture}_all_classification.json")

                # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ä½œæˆ - page_classifier.pyã®ã‚«ãƒ©ãƒ é †åºã«åˆã‚ã›ã‚‹
                csv_data = []
                for result in merged_data:
                    # è£œåŠ©é‡‘åˆ¶åº¦ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨URLã‚’æ–‡å­—åˆ—ã¨ã—ã¦çµåˆ
                    subsidies = result.get("found_new_housing_subsidies", [])
                    if subsidies:
                        titles = [
                            s.get("title", "") for s in subsidies if s.get("title")
                        ]
                        subsidy_urls = [
                            s.get("url", "") for s in subsidies if s.get("url")
                        ]
                        found_subsidies = " | ".join(titles)
                        subsidy_urls_str = " | ".join(subsidy_urls)
                    else:
                        found_subsidies = ""
                        subsidy_urls_str = ""

                    row = {
                        "URL": result.get("url", ""),
                        "éƒ½é“åºœçœŒ": result.get("prefecture", ""),
                        "å¸‚åŒºç”ºæ‘": result.get("city", ""),
                        "ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—": result.get("page_type", ""),
                        "å¯¾è±¡ãƒšãƒ¼ã‚¸": result.get("is_target_page", ""),
                        "ç¢ºä¿¡åº¦": result.get("confidence", 0.0),
                        "åˆ¤å®šç†ç”±": result.get("reasoning", ""),
                        "ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«": result.get("page_title", ""),
                        "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è¦ç´„": result.get("main_content_summary", ""),
                        "ã‚¨ãƒ©ãƒ¼": result.get("error", ""),
                        "è¦‹ã¤ã‹ã£ãŸè£œåŠ©é‡‘åˆ¶åº¦": found_subsidies,
                        "è£œåŠ©é‡‘åˆ¶åº¦URL": subsidy_urls_str,
                    }
                    csv_data.append(row)

                df_merged = pd.DataFrame(csv_data)
                csv_file = f"{prefecture}_all_classification.csv"
                df_merged.to_csv(csv_file, index=False, encoding="utf-8")
                print(f"âœ… çµ±åˆCSV: {csv_file}")

                created_finalize_file = True
    # ä½œæˆã•ã‚Œã¦ã„ãªã‹ã£ãŸã‚‰ã€æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ä½œæˆã™ã‚‹
    if not created_finalize_file:
        with open(f"{prefecture}_page_classification.json", "r") as f:
            data = json.load(f)
            # jsonã‚’ä½œã‚‹
            with open(f"{prefecture}_all_classification.json", "w") as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            print(f"âœ… çµ±åˆJSON: {prefecture}_all_classification.json")

            # csvã‚’ä½œã‚‹ - page_classifier.pyã®ã‚«ãƒ©ãƒ é †åºã«åˆã‚ã›ã‚‹
            csv_data = []
            for result in data:
                # è£œåŠ©é‡‘åˆ¶åº¦ã®ã‚¿ã‚¤ãƒˆãƒ«ã¨URLã‚’æ–‡å­—åˆ—ã¨ã—ã¦çµåˆ
                subsidies = result.get("found_new_housing_subsidies", [])
                if subsidies:
                    titles = [s.get("title", "") for s in subsidies if s.get("title")]
                    subsidy_urls = [s.get("url", "") for s in subsidies if s.get("url")]
                    found_subsidies = " | ".join(titles)
                    subsidy_urls_str = " | ".join(subsidy_urls)
                else:
                    found_subsidies = ""
                    subsidy_urls_str = ""

                row = {
                    "URL": result.get("url", ""),
                    "éƒ½é“åºœçœŒ": result.get("prefecture", ""),
                    "å¸‚åŒºç”ºæ‘": result.get("city", ""),
                    "ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—": result.get("page_type", ""),
                    "å¯¾è±¡ãƒšãƒ¼ã‚¸": result.get("is_target_page", ""),
                    "ç¢ºä¿¡åº¦": result.get("confidence", 0.0),
                    "åˆ¤å®šç†ç”±": result.get("reasoning", ""),
                    "ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«": result.get("page_title", ""),
                    "ã‚³ãƒ³ãƒ†ãƒ³ãƒ„è¦ç´„": result.get("main_content_summary", ""),
                    "ã‚¨ãƒ©ãƒ¼": result.get("error", ""),
                    "è¦‹ã¤ã‹ã£ãŸè£œåŠ©é‡‘åˆ¶åº¦": found_subsidies,
                    "è£œåŠ©é‡‘åˆ¶åº¦URL": subsidy_urls_str,
                }
                csv_data.append(row)

            pd_data = pd.DataFrame(csv_data)
            csv_file = f"{prefecture}_all_classification.csv"
            pd_data.to_csv(csv_file, index=False, encoding="utf-8")
            print(f"âœ… çµ±åˆCSV: {csv_file}")


if __name__ == "__main__":
    main()
