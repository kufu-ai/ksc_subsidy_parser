#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
åˆ†é¡çµæœã‚’ãƒãƒ¼ã‚¸ã—ã¦çµ±åˆçš„ãªå€‹åˆ¥ãƒšãƒ¼ã‚¸ãƒªã‚¹ãƒˆã‚’ä½œæˆã™ã‚‹ãƒ„ãƒ¼ãƒ«
"""

import pandas as pd
import json
import os
from pathlib import Path
from datetime import datetime
import re

def merge_classification_results(original_results, extracted_results):
    """
    å…ƒã®åˆ†é¡çµæœã¨æŠ½å‡ºçµæœã‚’ãƒãƒ¼ã‚¸

    Args:
        original_results (list): å…ƒã®åˆ†é¡çµæœ
        extracted_results (list): ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡ºã—ãŸçµæœ

    Returns:
        dict: ãƒãƒ¼ã‚¸ã•ã‚ŒãŸçµæœ
    """
    # å€‹åˆ¥ãƒšãƒ¼ã‚¸ã®ã¿ã‚’æŠ½å‡ºï¼ˆæ–°ã‚¹ã‚­ãƒ¼ãƒã®ã¿ï¼‰
    original_individual = [r for r in original_results if r.get('page_type') == 'æ–°ç¯‰ä½å®…é–¢é€£å€‹åˆ¥ãƒšãƒ¼ã‚¸']
    extracted_individual = [r for r in extracted_results if r.get('page_type') == 'æ–°ç¯‰ä½å®…é–¢é€£å€‹åˆ¥ãƒšãƒ¼ã‚¸']

    # å…ƒã®å€‹åˆ¥ãƒšãƒ¼ã‚¸ã«ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¿½åŠ 
    for page in original_individual:
        page['source'] = 'åˆå›æ¤œç´¢'
        page['extracted_from_list'] = False

    # æŠ½å‡ºã—ãŸå€‹åˆ¥ãƒšãƒ¼ã‚¸ã«ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’è¿½åŠ 
    for page in extracted_individual:
        page['source'] = 'ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡º'
        # extracted_from_listã¯æ—¢ã«è¨­å®šæ¸ˆã¿

    # URLé‡è¤‡ã‚’é™¤å»ã—ãªãŒã‚‰ãƒãƒ¼ã‚¸
    seen_urls = set()
    merged_individual = []

    # å…ƒã®çµæœã‚’å…ˆã«è¿½åŠ ï¼ˆå„ªå…ˆåº¦é«˜ï¼‰
    for page in original_individual:
        url = page.get('url', '')
        if url and url not in seen_urls:
            seen_urls.add(url)
            merged_individual.append(page)

    # æŠ½å‡ºçµæœã‚’è¿½åŠ ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼‰
    new_from_extraction = 0
    for page in extracted_individual:
        url = page.get('url', '')
        if url and url not in seen_urls:
            seen_urls.add(url)
            merged_individual.append(page)
            new_from_extraction += 1

    return {
        'merged_individual_pages': merged_individual,
        'statistics': {
            'original_count': len(original_individual),
            'extracted_count': len(extracted_individual),
            'merged_count': len(merged_individual),
            'new_from_extraction': new_from_extraction,
            'duplicate_removed': len(original_individual) + len(extracted_individual) - len(merged_individual)
        }
    }

def create_comprehensive_summary(merged_data):
    """
    åŒ…æ‹¬çš„ãªã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ

    Args:
        merged_data (dict): ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿

    Returns:
        dict: ã‚µãƒãƒªãƒ¼ãƒ‡ãƒ¼ã‚¿
    """
    individual_pages = merged_data['merged_individual_pages']

    # éƒ½é“åºœçœŒãƒ»å¸‚åŒºç”ºæ‘åˆ¥çµ±è¨ˆ
    prefecture_stats = {}
    city_stats = {}
    source_stats = {'åˆå›æ¤œç´¢': 0, 'ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡º': 0}

    for page in individual_pages:
        # éƒ½é“åºœçœŒçµ±è¨ˆï¼ˆparent_prefectureã‚‚è€ƒæ…®ï¼‰
        pref = page.get('prefecture') or page.get('parent_prefecture', 'ä¸æ˜')
        prefecture_stats[pref] = prefecture_stats.get(pref, 0) + 1

        # å¸‚åŒºç”ºæ‘çµ±è¨ˆ
        city = page.get('city') or page.get('parent_city', 'ä¸æ˜')
        city_key = f"{pref} {city}"
        city_stats[city_key] = city_stats.get(city_key, 0) + 1

        # ã‚½ãƒ¼ã‚¹çµ±è¨ˆ
        source = page.get('source', 'ä¸æ˜')
        if source in source_stats:
            source_stats[source] += 1

    # ç¢ºä¿¡åº¦çµ±è¨ˆ
    confidences = [p.get('confidence', 0.0) for p in individual_pages if p.get('confidence') is not None]
    confidence_stats = {}
    if confidences:
        confidence_stats = {
            'average': sum(confidences) / len(confidences),
            'max': max(confidences),
            'min': min(confidences),
            'high_confidence_count': len([c for c in confidences if c >= 0.8])
        }

    # è£œåŠ©é‡‘åˆ¶åº¦çµ±è¨ˆ
    all_titles = []
    for page in individual_pages:
        titles = page.get('found_subsidy_titles', [])
        all_titles.extend(titles)

    unique_titles = list(set(all_titles))

    return {
        'total_individual_pages': len(individual_pages),
        'prefecture_stats': dict(sorted(prefecture_stats.items(), key=lambda x: x[1], reverse=True)),
        'city_stats': dict(sorted(city_stats.items(), key=lambda x: x[1], reverse=True)[:20]),  # ä¸Šä½20å¸‚åŒºç”ºæ‘
        'source_stats': source_stats,
        'confidence_stats': confidence_stats,
        'subsidy_titles_found': len(unique_titles),
        'sample_titles': unique_titles[:10]  # ã‚µãƒ³ãƒ—ãƒ«ã‚¿ã‚¤ãƒˆãƒ«
    }

def save_merged_results(merged_data, comprehensive_summary, base_filename):
    """
    ãƒãƒ¼ã‚¸çµæœã‚’ä¿å­˜

    Args:
        merged_data (dict): ãƒãƒ¼ã‚¸ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        comprehensive_summary (dict): åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼
        base_filename (str): ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«å
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    individual_pages = merged_data['merged_individual_pages']

    # 1. çµ±åˆå€‹åˆ¥ãƒšãƒ¼ã‚¸URLãƒªã‚¹ãƒˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰
    merged_urls_file = f"{base_filename}_merged_individual_urls.txt"
    with open(merged_urls_file, 'w', encoding='utf-8') as f:
        for page in individual_pages:
            f.write(f"{page.get('url', '')}\n")
    print(f"âœ… çµ±åˆURLãƒªã‚¹ãƒˆ: {merged_urls_file} ({len(individual_pages)}ä»¶)")

    # 2. è©³ç´°æƒ…å ±ä»˜ãJSON
    merged_detailed_file = f"{base_filename}_merged_individual_detailed.json"
    with open(merged_detailed_file, 'w', encoding='utf-8') as f:
        json.dump(individual_pages, f, ensure_ascii=False, indent=2)
    print(f"âœ… çµ±åˆè©³ç´°æƒ…å ±: {merged_detailed_file}")

    # 3. CSVå½¢å¼
    df_merged = pd.DataFrame(individual_pages)
    merged_csv_file = f"{base_filename}_merged_individual.csv"
    df_merged.to_csv(merged_csv_file, index=False, encoding='utf-8')
    print(f"âœ… çµ±åˆCSV: {merged_csv_file}")

    # 4. éƒ½é“åºœçœŒãƒ»å¸‚åŒºç”ºæ‘åˆ¥ã‚µãƒãƒªãƒ¼
    summary_data = []
    for city_key, count in comprehensive_summary['city_stats'].items():
        try:
            pref, city = city_key.split(' ', 1)
        except ValueError:
            pref, city = city_key, ''

        # ãã®å¸‚åŒºç”ºæ‘ã®å€‹åˆ¥ãƒšãƒ¼ã‚¸ã‚’å–å¾—
        city_pages = [p for p in individual_pages
                     if (p.get('prefecture') or p.get('parent_prefecture', '')) == pref
                     and (p.get('city') or p.get('parent_city', '')) == city]

        # çµ±è¨ˆè¨ˆç®—
        sources = [p.get('source', 'ä¸æ˜') for p in city_pages]
        source_counts = {s: sources.count(s) for s in set(sources)}

        confidences = [p.get('confidence', 0.0) for p in city_pages if p.get('confidence') is not None]
        avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0

        # è£œåŠ©é‡‘åˆ¶åº¦ã‚¿ã‚¤ãƒˆãƒ«
        titles = []
        for p in city_pages:
            titles.extend(p.get('found_subsidy_titles', []))
        unique_titles = list(set(titles))

        summary_data.append({
            'éƒ½é“åºœçœŒå': pref,
            'å¸‚åŒºç”ºæ‘å': city,
            'å€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°': count,
            'åˆå›æ¤œç´¢ç”±æ¥': source_counts.get('åˆå›æ¤œç´¢', 0),
            'ä¸€è¦§ãƒšãƒ¼ã‚¸æŠ½å‡ºç”±æ¥': source_counts.get('ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡º', 0),
            'å¹³å‡ç¢ºä¿¡åº¦': round(avg_confidence, 3),
            'è¦‹ã¤ã‹ã£ãŸè£œåŠ©é‡‘åˆ¶åº¦æ•°': len(unique_titles),
            'è£œåŠ©é‡‘åˆ¶åº¦ä¾‹': ', '.join(unique_titles[:3]),
            'URLä¾‹': city_pages[0].get('url', '') if city_pages else '',
        })

    summary_csv_file = f"{base_filename}_merged_summary.csv"
    df_summary = pd.DataFrame(summary_data)
    df_summary.to_csv(summary_csv_file, index=False, encoding='utf-8')
    print(f"âœ… çµ±åˆã‚µãƒãƒªãƒ¼: {summary_csv_file}")

    # 5. çµ±è¨ˆæƒ…å ±
    stats_data = {
        'merge_timestamp': timestamp,
        'merge_statistics': merged_data['statistics'],
        'comprehensive_summary': comprehensive_summary
    }

    stats_file = f"{base_filename}_merged_stats.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats_data, f, ensure_ascii=False, indent=2)
    print(f"âœ… çµ±è¨ˆæƒ…å ±: {stats_file}")

    # éƒ½é“åºœçœŒâ†’å¸‚åŒºç”ºæ‘â†’URLãƒªã‚¹ãƒˆï¼ˆã‚¿ã‚¤ãƒˆãƒ«ä»˜ãï¼‰å½¢å¼ã®JSONå‡ºåŠ›
    # TODO: å†…å®¹ã®è¦ç´„æ™‚ã«ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚prefectureã¨cityãŒæ©Ÿæ¢°çš„ã«åˆ¤æ–­ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã®ã§AIã«ã‚ˆã‚‹é–“é•ãˆãŒèµ·ã“ã‚‰ãªã„ããªã‚‹ã€‚
    # individual_pagesã‚’éƒ½é“åºœçœŒãƒ»å¸‚åŒºç”ºæ‘ã”ã¨ã«ã¾ã¨ã‚ã‚‹
    pref_city_dict = {}
    for page in individual_pages:
        pref = page.get('prefecture') or page.get('parent_prefecture', 'ä¸æ˜')
        city = page.get('city') or page.get('parent_city', 'ä¸æ˜')
        # URLã¨ã‚¿ã‚¤ãƒˆãƒ«ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
        urls = []
        # found_new_housing_subsidiesãŒã‚ã‚Œã°ãã®title/urlã€ãªã‘ã‚Œã°page_title
        if page.get('found_new_housing_subsidies'):
            for sub in page['found_new_housing_subsidies']:
                urls.append({
                    'url': sub.get('url', page.get('url', '')),
                    'title': sub.get('title', page.get('page_title', ''))
                })
        else:
            urls.append({
                'url': page.get('url', ''),
                'title': page.get('page_title', '')
            })
        # æ—¢å­˜ã®å¸‚åŒºç”ºæ‘ã‚¨ãƒ³ãƒˆãƒªã‚’æ¢ã™
        if pref not in pref_city_dict:
            pref_city_dict[pref] = []
        # city_nameãŒæ—¢ã«ã‚ã‚Œã°urlsã‚’è¿½åŠ ã€ãªã‘ã‚Œã°æ–°è¦
        found = False
        for city_entry in pref_city_dict[pref]:
            if city_entry['city_name'] == city:
                city_entry['urls'].extend(urls)
                found = True
                break
        if not found:
            pref_city_dict[pref].append({
                'city_name': city,
                'urls': urls
            })
    city_urls_json_file = f"{base_filename}_merged_city_urls.json"
    with open(city_urls_json_file, 'w', encoding='utf-8') as f:
        json.dump(pref_city_dict, f, ensure_ascii=False, indent=2)
    print(f"âœ… éƒ½é“åºœçœŒâ†’å¸‚åŒºç”ºæ‘â†’URLãƒªã‚¹ãƒˆJSON: {city_urls_json_file}")

def print_merge_statistics(merged_data, comprehensive_summary):
    """
    ãƒãƒ¼ã‚¸çµ±è¨ˆã‚’è¡¨ç¤º
    """
    merge_stats = merged_data['statistics']

    print(f"\n{'='*60}")
    print(f"ğŸ“Š åˆ†é¡çµæœãƒãƒ¼ã‚¸çµ±è¨ˆ")
    print(f"{'='*60}")

    print(f"å…ƒã®å€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°: {merge_stats['original_count']}")
    print(f"ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰æŠ½å‡ºã—ãŸå€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°: {merge_stats['extracted_count']}")
    print(f"é‡è¤‡é™¤å»å¾Œã®çµ±åˆå€‹åˆ¥ãƒšãƒ¼ã‚¸æ•°: {merge_stats['merged_count']}")
    print(f"æ–°è¦ç™ºè¦‹ãƒšãƒ¼ã‚¸æ•°: {merge_stats['new_from_extraction']}")
    print(f"é™¤å»ã•ã‚ŒãŸé‡è¤‡æ•°: {merge_stats['duplicate_removed']}")

    print(f"\n--- ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ ---")
    for source, count in comprehensive_summary['source_stats'].items():
        print(f"{source}: {count}ä»¶")

    print(f"\n--- éƒ½é“åºœçœŒåˆ¥ä¸Šä½10 ---")
    for i, (pref, count) in enumerate(list(comprehensive_summary['prefecture_stats'].items())[:10], 1):
        print(f"{i:2d}. {pref}: {count}ä»¶")

    if comprehensive_summary['confidence_stats']:
        conf_stats = comprehensive_summary['confidence_stats']
        print(f"\n--- ç¢ºä¿¡åº¦çµ±è¨ˆ ---")
        print(f"å¹³å‡: {conf_stats['average']:.3f}")
        print(f"æœ€é«˜: {conf_stats['max']:.3f}")
        print(f"æœ€ä½: {conf_stats['min']:.3f}")
        print(f"é«˜ç¢ºä¿¡åº¦(â‰¥0.8): {conf_stats['high_confidence_count']}ä»¶")

    print(f"\n--- è£œåŠ©é‡‘åˆ¶åº¦ç™ºè¦‹çµ±è¨ˆ ---")
    print(f"ç™ºè¦‹ã•ã‚ŒãŸè£œåŠ©é‡‘åˆ¶åº¦æ•°: {comprehensive_summary['subsidy_titles_found']}")
    if comprehensive_summary['sample_titles']:
        print(f"åˆ¶åº¦ä¾‹: {', '.join(comprehensive_summary['sample_titles'][:5])}")

def find_result_files():
    """
    çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    """
    files = {
        'classification': [],
        'extraction': []
    }

    # åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«
    for pattern in ['*_page_classification.json']:
        files['classification'].extend(Path('.').glob(pattern))

    # æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«
    for pattern in ['*_extracted_all.json']:
        files['extraction'].extend(Path('.').glob(pattern))

    return files

def load_json_file(file_path):
    """
    JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ ({file_path}): {str(e)}")
        return []

def main():
    """
    ãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    print("ğŸ”— åˆ†é¡çµæœãƒãƒ¼ã‚¸ãƒ„ãƒ¼ãƒ«")
    print("-" * 40)

    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
    result_files = find_result_files()

    if not result_files['classification']:
        print("âŒ åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ (*_page_classification.json)")
        return

    print("åˆ©ç”¨å¯èƒ½ãªåˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«:")
    for i, file in enumerate(result_files['classification'], 1):
        print(f"{i}. {file.name}")

    try:
        choice = int(input("\nãƒ™ãƒ¼ã‚¹ã¨ãªã‚‹åˆ†é¡çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„: ")) - 1
        if not (0 <= choice < len(result_files['classification'])):
            print("âŒ ç„¡åŠ¹ãªé¸æŠã§ã™")
            return

        classification_file = result_files['classification'][choice]
        base_name = classification_file.stem.replace('_page_classification', '')

        print(f"\nğŸ“‚ {classification_file.name} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        original_results = load_json_file(str(classification_file))

        if not original_results:
            print("âŒ åˆ†é¡çµæœãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ")
            return

        # å¯¾å¿œã™ã‚‹æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        extraction_file = Path(f"{base_name}_extracted_all.json")

        if extraction_file.exists():
            print(f"ğŸ“‚ {extraction_file.name} ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            extracted_results = load_json_file(str(extraction_file))
        else:
            print("âš ï¸  å¯¾å¿œã™ã‚‹æŠ½å‡ºçµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ƒã®çµæœã®ã¿ã‚’å‡¦ç†ã—ã¾ã™ã€‚")
            extracted_results = []

        print(f"\nğŸ”„ çµæœã‚’ãƒãƒ¼ã‚¸ä¸­...")

        # çµæœã‚’ãƒãƒ¼ã‚¸
        merged_data = merge_classification_results(original_results, extracted_results)

        # åŒ…æ‹¬çš„ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
        comprehensive_summary = create_comprehensive_summary(merged_data)

        # çµæœã‚’ä¿å­˜
        save_merged_results(merged_data, comprehensive_summary, base_name)

        # çµ±è¨ˆã‚’è¡¨ç¤º
        print_merge_statistics(merged_data, comprehensive_summary)

        print(f"\nğŸ‰ ãƒãƒ¼ã‚¸å®Œäº†ï¼")
        print(f"ğŸ“ çµ±åˆå€‹åˆ¥ãƒšãƒ¼ã‚¸ãƒªã‚¹ãƒˆ: {base_name}_merged_individual_urls.txt")

    except ValueError:
        print("âŒ ç„¡åŠ¹ãªå…¥åŠ›ã§ã™")
    except KeyboardInterrupt:
        print("\nâš ï¸  å‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {str(e)}")

if __name__ == '__main__':
    main()
